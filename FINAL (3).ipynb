{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwZZhejhwUnD0Ym0jatjzT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vDywWDViIt3E"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy\n","torch.manual_seed(42)\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512, dropout=0):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)#d_model is the the dimension of the embedding,maxlen-is the maximum length of sequence\n","        position = torch.arange(0, max_len).unsqueeze(1).float() #shape('max_len,1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) #calculates the positional encodings using the formula\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)#even position\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)#odd position\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return self.encoding[:, :x.size(1)].detach()\n","\n","class TransformerModelLogger:\n","  def __init__(self, is_logging=False):\n","    self.is_logging = is_logging\n","    self.logs = []\n","\n","  def log(self, message):\n","    if self.is_logging:\n","      self.logs.append(message)\n","      print(message)\n","\n","  def print_logs(self):\n","     for message in self.logs:\n","\n","      print(message)\n","\n","      self.logs = []\n","\n","class TransformerModel1(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n","\n","        super(TransformerModel1, self).__init__()\n","\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n","\n","        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n","        self.logger = TransformerModelLogger(is_logging=True)\n","\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout,\n","            dim_feedforward=d_ff,\n","        )\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","\n","    def generate_mask(self, src, tgt):\n","\n","        src_mask = None\n","        seq_length = tgt.size(0)\n","\n","        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()#triangular matrix used to mask future positiions of the target sequence\n","\n","        return src_mask, nopeak_mask\n","\n","    def forward(self, src, tgt):\n","\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","\n","        self.logger.log(\"Tgt mask shape = \" + str(tgt_mask.shape))\n","\n","        src = self.src_embedding(src) + self.positional_encoding(src)\n","        self.logger.log(f\"src (after embedding and positional encoding): {src.shape}\")\n","        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n","        self.logger.log(f\"tgt (after embedding and positional encoding): {tgt.shape}\")\n","\n","\n","        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n","        self.logger.log(f\"output (after transformer): {output.shape}\")\n","        output = self.fc(output)\n","        print(\"output:{output}\")\n","\n","        return output\n","\n"],"metadata":{"id":"sr7P9D2pU0kg","executionInfo":{"status":"ok","timestamp":1718597213733,"user_tz":-330,"elapsed":4259,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","src_vocab_size = 20\n","tgt_vocab_size = 20\n","d_model = 16\n","num_heads = 4\n","num_encoder_layers = 2\n","num_decoder_layers = 2\n","d_ff = 20\n","max_seq_len = 5\n","dropout = 0\n","\n","transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n","\n","\n","\n","src_data = torch.tensor([[2], [1], [5], [4]])\n","tgt_data = torch.tensor([[1], [16], [5], [3]])\n","\n","\n","state_dict = transformer.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVF8VAz-U0nZ","executionInfo":{"status":"ok","timestamp":1718598504733,"user_tz":-330,"elapsed":478,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"2f11a3c7-8770-4801-d20a-d55706a54a96"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["import copy\n","\n","state_dict1 = copy.deepcopy(state_dict)"],"metadata":{"id":"apu_yD89U0qL","executionInfo":{"status":"ok","timestamp":1718597220947,"user_tz":-330,"elapsed":457,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["transformer.train()\n","output = transformer(src_data, tgt_data)\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joPRFMTlU0sy","executionInfo":{"status":"ok","timestamp":1718597222913,"user_tz":-330,"elapsed":39,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"8f2b5ad8-5b4c-4408-dc40-d2e4803c6015"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Tgt mask shape = torch.Size([4, 4])\n","src (after embedding and positional encoding): torch.Size([4, 1, 16])\n","tgt (after embedding and positional encoding): torch.Size([4, 1, 16])\n","inital input for else  tensor([[[-1.3847,  0.1288, -0.2234,  2.7174,  0.3189,  0.5755,  0.3057,\n","           0.2254, -1.5576,  1.9956, -0.8798,  0.3989, -1.2742,  3.1228,\n","          -1.2347,  0.5121]],\n","\n","        [[ 1.6423,  0.8404, -0.4974,  1.4396, -0.7581,  2.0783,  0.8008,\n","           2.6806,  1.2791,  2.2964,  0.6105,  2.3347, -0.2316,  1.0418,\n","          -0.2516,  1.8599]],\n","\n","        [[ 0.0109,  0.6613, -1.3407,  0.4146,  0.5362,  1.5246,  1.1412,\n","           1.0516,  0.7440,  0.5184, -1.0495,  1.6039, -1.7223,  0.1722,\n","           1.3347,  1.4835]],\n","\n","        [[ 1.4451,  1.8564,  2.2181,  1.5232,  0.3466,  0.8027, -1.0546,\n","           2.2780, -0.1722,  1.5238,  0.0566,  1.4263,  0.5750,  0.3583,\n","          -2.2064,  0.2492]]], grad_fn=<AddBackward0>)\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[-1.2904, -1.4548,  0.2409, -1.9437, -0.7866, -0.0888, -0.8894,\n","           0.4835, -0.6765, -1.1213, -0.2185,  0.6928, -0.1620, -1.5521,\n","           0.3123,  1.1321]],\n","\n","        [[-0.3896,  0.6897, -0.0314,  0.1926, -0.3449,  0.8441, -1.5034,\n","          -1.3316,  1.7504, -2.5148,  0.4080,  0.4009,  0.1403, -1.2915,\n","          -0.3278, -1.4851]],\n","\n","        [[ 0.1109,  0.6075,  0.4831,  0.6146,  0.0981, -0.0047, -0.2428,\n","          -0.7894,  0.3918, -1.0184,  0.4244,  0.6083,  0.1127, -1.1332,\n","          -1.7520, -2.3310]],\n","\n","        [[ 0.3891, -0.3911, -0.2980, -0.7874, -0.0850,  1.3103, -0.7144,\n","           0.8460, -0.4587, -1.6422, -0.5962,  0.8148,  0.2518, -1.7518,\n","           0.6888,  0.9547]]], grad_fn=<SelectBackward0>)\n","\n","key matrix :-  tensor([[[ 1.3129, -0.3780, -0.1206,  0.0688, -0.4421, -0.4934,  0.0395,\n","           1.7299, -1.3354,  0.6869,  1.6883,  0.9355, -0.8966, -2.4217,\n","          -0.6368,  0.4559]],\n","\n","        [[-0.5893,  0.1617, -0.4476, -2.3128, -2.4724,  1.1490,  0.5380,\n","           0.6844,  0.4416,  0.2506,  1.5647, -0.3888,  1.8197, -0.3332,\n","           0.8492,  0.3809]],\n","\n","        [[-0.6759,  0.5452, -0.9848, -0.0179, -1.6523,  0.3462, -0.2870,\n","           0.4872, -0.0510,  0.4742,  0.5298,  0.4497, -0.5066, -0.3104,\n","           0.7142,  0.6256]],\n","\n","        [[ 0.5205, -0.0398,  0.1450, -2.5935, -0.6763,  1.6335, -0.0379,\n","           0.9320,  1.0452,  0.3181,  0.1340,  0.2590,  2.1055, -0.2493,\n","          -0.3929,  0.0998]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[ 1.1858,  1.1215, -1.2000,  0.5648,  0.8549,  0.5353, -0.2303,\n","          -0.3514, -0.2159, -1.4867, -0.7714, -1.6448,  0.3498, -0.4657,\n","           0.3006,  1.1790]],\n","\n","        [[ 1.3794,  1.2286, -0.7844, -0.9318,  1.2872,  1.0075,  0.6375,\n","          -0.6809, -0.3153, -0.7692, -2.3214, -0.1495,  0.6325, -0.6396,\n","           0.4989,  1.0050]],\n","\n","        [[ 0.9299,  1.2608, -1.5720,  0.3946,  0.4537,  0.9049,  0.6801,\n","           0.1284, -0.1711, -0.2631, -1.5285, -0.6780,  0.2116, -0.1656,\n","          -0.2647,  0.7863]],\n","\n","        [[ 0.9974,  0.0138, -0.2135, -0.2077,  0.7062, -1.0058, -0.1952,\n","          -0.5689, -0.7690, -0.3978, -0.5688,  0.1114,  1.2340, -0.7559,\n","           1.7006, -0.3825]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[-1.2904, -1.4548,  0.2409, -1.9437],\n","          [-0.3896,  0.6897, -0.0314,  0.1926],\n","          [ 0.1109,  0.6075,  0.4831,  0.6146],\n","          [ 0.3891, -0.3911, -0.2980, -0.7874]],\n","\n","         [[-0.7866, -0.0888, -0.8894,  0.4835],\n","          [-0.3449,  0.8441, -1.5034, -1.3316],\n","          [ 0.0981, -0.0047, -0.2428, -0.7894],\n","          [-0.0850,  1.3103, -0.7144,  0.8460]],\n","\n","         [[-0.6765, -1.1213, -0.2185,  0.6928],\n","          [ 1.7504, -2.5148,  0.4080,  0.4009],\n","          [ 0.3918, -1.0184,  0.4244,  0.6083],\n","          [-0.4587, -1.6422, -0.5962,  0.8148]],\n","\n","         [[-0.1620, -1.5521,  0.3123,  1.1321],\n","          [ 0.1403, -1.2915, -0.3278, -1.4851],\n","          [ 0.1127, -1.1332, -1.7520, -2.3310],\n","          [ 0.2518, -1.7518,  0.6888,  0.9547]]]], grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[ 1.3129, -0.3780, -0.1206,  0.0688],\n","          [-0.5893,  0.1617, -0.4476, -2.3128],\n","          [-0.6759,  0.5452, -0.9848, -0.0179],\n","          [ 0.5205, -0.0398,  0.1450, -2.5935]],\n","\n","         [[-0.4421, -0.4934,  0.0395,  1.7299],\n","          [-2.4724,  1.1490,  0.5380,  0.6844],\n","          [-1.6523,  0.3462, -0.2870,  0.4872],\n","          [-0.6763,  1.6335, -0.0379,  0.9320]],\n","\n","         [[-1.3354,  0.6869,  1.6883,  0.9355],\n","          [ 0.4416,  0.2506,  1.5647, -0.3888],\n","          [-0.0510,  0.4742,  0.5298,  0.4497],\n","          [ 1.0452,  0.3181,  0.1340,  0.2590]],\n","\n","         [[-0.8966, -2.4217, -0.6368,  0.4559],\n","          [ 1.8197, -0.3332,  0.8492,  0.3809],\n","          [-0.5066, -0.3104,  0.7142,  0.6256],\n","          [ 2.1055, -0.2493, -0.3929,  0.0998]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[ 1.1858,  1.1215, -1.2000,  0.5648],\n","          [ 1.3794,  1.2286, -0.7844, -0.9318],\n","          [ 0.9299,  1.2608, -1.5720,  0.3946],\n","          [ 0.9974,  0.0138, -0.2135, -0.2077]],\n","\n","         [[ 0.8549,  0.5353, -0.2303, -0.3514],\n","          [ 1.2872,  1.0075,  0.6375, -0.6809],\n","          [ 0.4537,  0.9049,  0.6801,  0.1284],\n","          [ 0.7062, -1.0058, -0.1952, -0.5689]],\n","\n","         [[-0.2159, -1.4867, -0.7714, -1.6448],\n","          [-0.3153, -0.7692, -2.3214, -0.1495],\n","          [-0.1711, -0.2631, -1.5285, -0.6780],\n","          [-0.7690, -0.3978, -0.5688,  0.1114]],\n","\n","         [[ 0.3498, -0.4657,  0.3006,  1.1790],\n","          [ 0.6325, -0.6396,  0.4989,  1.0050],\n","          [ 0.2116, -0.1656, -0.2647,  0.7863],\n","          [ 1.2340, -0.7559,  1.7006, -0.3825]]]], grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[ 1.1975,  0.7233, -0.5901, -0.5410],\n","          [ 1.1046,  0.9953, -1.0495, -0.0259],\n","          [ 1.1024,  0.9936, -1.0832,  0.1348],\n","          [ 1.1383,  0.7302, -0.7275, -0.2136]],\n","\n","         [[ 0.8317,  0.4873,  0.2970, -0.3451],\n","          [ 0.7831,  0.3065,  0.3315, -0.3397],\n","          [ 0.7998,  0.3635,  0.2687, -0.3422],\n","          [ 0.8279,  0.0524,  0.1601, -0.4431]],\n","\n","         [[-0.3279, -0.8337, -1.1921, -0.7992],\n","          [-0.4901, -0.5620, -1.3250, -0.1991],\n","          [-0.3794, -0.7244, -1.3107, -0.5530],\n","          [-0.3664, -0.7258, -1.1880, -0.6542]],\n","\n","         [[ 0.4312, -0.4609,  0.3427,  0.9712],\n","          [ 0.5600, -0.5204,  0.5627,  0.7894],\n","          [ 0.5946, -0.5391,  0.6550,  0.7374],\n","          [ 0.4796, -0.4850,  0.4036,  0.9155]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","After self-attention block: tensor([[[ 0.4032,  0.6285,  0.3654, -0.0856, -0.6545, -1.1837,  0.3380,\n","          -0.2964,  0.2882, -0.5752, -0.5372,  0.5484, -1.0267,  0.5842,\n","          -0.2027, -0.2501]],\n","\n","        [[ 0.6164,  0.9081,  0.2783,  0.2989, -0.9625, -1.1206,  0.0936,\n","          -0.5293,  0.7180, -0.7569, -0.6895,  0.4782, -1.3156,  0.3045,\n","          -0.4648, -0.4918]],\n","\n","        [[ 0.5592,  1.0481,  0.2919,  0.1912, -0.9777, -1.1585,  0.1585,\n","          -0.4571,  0.6582, -0.6280, -0.7796,  0.4591, -1.1811,  0.3573,\n","          -0.3197, -0.5860]],\n","\n","        [[ 0.3656,  0.6904,  0.1486, -0.0918, -0.5908, -1.1434,  0.3054,\n","          -0.1575,  0.3484, -0.5267, -0.5138,  0.5082, -0.9418,  0.5130,\n","          -0.3578, -0.2816]]], grad_fn=<ViewBackward0>)\n","After adding self-attention output to x: tensor([[[-0.9815,  0.7572,  0.1420,  2.6318, -0.3357, -0.6082,  0.6437,\n","          -0.0710, -1.2694,  1.4204, -1.4170,  0.9473, -2.3009,  3.7070,\n","          -1.4374,  0.2620]],\n","\n","        [[ 2.2587,  1.7485, -0.2191,  1.7385, -1.7207,  0.9577,  0.8944,\n","           2.1513,  1.9972,  1.5395, -0.0790,  2.8129, -1.5472,  1.3463,\n","          -0.7163,  1.3680]],\n","\n","        [[ 0.5701,  1.7094, -1.0488,  0.6058, -0.4415,  0.3661,  1.2997,\n","           0.5945,  1.4021, -0.1096, -1.8291,  2.0630, -2.9034,  0.5296,\n","           1.0150,  0.8975]],\n","\n","        [[ 1.8108,  2.5468,  2.3666,  1.4314, -0.2441, -0.3407, -0.7492,\n","           2.1205,  0.1762,  0.9971, -0.4572,  1.9345, -0.3668,  0.8712,\n","          -2.5642, -0.0324]]], grad_fn=<AddBackward0>)\n","After norm1: tensor([[[-0.7332,  0.4131,  0.0075,  1.6488, -0.3074, -0.4871,  0.3382,\n","          -0.1329, -0.9229,  0.8502, -1.0203,  0.5384, -1.6029,  2.3576,\n","          -1.0337,  0.0866]],\n","\n","        [[ 1.0171,  0.6329, -0.8490,  0.6253, -1.9799,  0.0373, -0.0104,\n","           0.9363,  0.8202,  0.4755, -0.7435,  1.4346, -1.8493,  0.3300,\n","          -1.2235,  0.3463]],\n","\n","        [[ 0.2166,  1.1134, -1.0579,  0.2446, -0.5798,  0.0560,  0.7909,\n","           0.2358,  0.8715, -0.3185, -1.6721,  1.3918, -2.5179,  0.1846,\n","           0.5668,  0.4743]],\n","\n","        [[ 0.8970,  1.4394,  1.3067,  0.6173, -0.6176, -0.6888, -0.9898,\n","           1.1253, -0.3078,  0.2973, -0.7746,  0.9882, -0.7080,  0.2045,\n","          -2.3275, -0.4615]]], grad_fn=<NativeLayerNormBackward0>)\n","After feed-forward block: tensor([[[ 1.6597,  0.7196, -0.4650,  0.6206, -0.1168,  1.5170,  0.3761,\n","          -0.5452, -0.2741, -0.3194,  0.2897, -0.1310,  0.9299,  0.6972,\n","           0.6831,  0.4299]],\n","\n","        [[ 1.1977,  0.2741, -0.3182,  0.1898, -0.3302,  1.5173, -0.4644,\n","          -0.3026, -0.5390, -0.2433,  1.9145, -0.4556,  1.0266,  0.1808,\n","           1.5084,  0.4692]],\n","\n","        [[ 1.3498,  1.7038,  0.7465,  1.6802,  0.5069,  1.6269, -0.2045,\n","          -0.4474, -0.0904, -0.6766,  2.2934, -0.4881,  1.0865, -0.0052,\n","           0.9247, -0.3708]],\n","\n","        [[ 1.2659, -0.3442,  0.1237, -0.1911,  0.2307,  0.5551, -0.2112,\n","           0.6098, -0.9191,  0.0466,  0.3066,  0.3549,  0.5601,  1.3241,\n","           0.2828,  0.1599]]], grad_fn=<ViewBackward0>)\n","After adding feed-forward output to x: tensor([[[ 0.9265,  1.1327, -0.4575,  2.2695, -0.4243,  1.0299,  0.7143,\n","          -0.6781, -1.1970,  0.5309, -0.7305,  0.4074, -0.6730,  3.0548,\n","          -0.3506,  0.5165]],\n","\n","        [[ 2.2149,  0.9069, -1.1672,  0.8151, -2.3101,  1.5546, -0.4748,\n","           0.6336,  0.2811,  0.2322,  1.1710,  0.9789, -0.8226,  0.5108,\n","           0.2849,  0.8156]],\n","\n","        [[ 1.5664,  2.8172, -0.3115,  1.9249, -0.0729,  1.6829,  0.5864,\n","          -0.2117,  0.7812, -0.9951,  0.6212,  0.9036, -1.4314,  0.1795,\n","           1.4915,  0.1035]],\n","\n","        [[ 2.1629,  1.0952,  1.4304,  0.4262, -0.3869, -0.1337, -1.2011,\n","           1.7350, -1.2269,  0.3439, -0.4680,  1.3431, -0.1479,  1.5286,\n","          -2.0447, -0.3016]]], grad_fn=<AddBackward0>)\n","After norm2: tensor([[[ 0.4893,  0.6737, -0.7487,  1.6906, -0.7189,  0.5818,  0.2995,\n","          -0.9460, -1.4102,  0.1354, -0.9929,  0.0250, -0.9415,  2.3931,\n","          -0.6530,  0.1226]],\n","\n","        [[ 1.7421,  0.5192, -1.4199,  0.4334, -2.4885,  1.1248, -0.7726,\n","           0.2637, -0.0659, -0.1116,  0.7661,  0.5865, -1.0978,  0.1489,\n","          -0.0623,  0.4338]],\n","\n","        [[ 0.8910,  2.0469, -0.8444,  1.2223, -0.6239,  0.9987, -0.0146,\n","          -0.7522,  0.1654, -1.4761,  0.0176,  0.2785, -1.8793, -0.3907,\n","           0.8218, -0.4609]],\n","\n","        [[ 1.6259,  0.7138,  1.0001,  0.1423, -0.5523, -0.3360, -1.2479,\n","           1.2604, -1.2699,  0.0720, -0.6216,  0.9255, -0.3482,  1.0841,\n","          -1.9686, -0.4795]]], grad_fn=<NativeLayerNormBackward0>)\n","inital input for else  tensor([[[ 0.4893,  0.6737, -0.7487,  1.6906, -0.7189,  0.5818,  0.2995,\n","          -0.9460, -1.4102,  0.1354, -0.9929,  0.0250, -0.9415,  2.3931,\n","          -0.6530,  0.1226]],\n","\n","        [[ 1.7421,  0.5192, -1.4199,  0.4334, -2.4885,  1.1248, -0.7726,\n","           0.2637, -0.0659, -0.1116,  0.7661,  0.5865, -1.0978,  0.1489,\n","          -0.0623,  0.4338]],\n","\n","        [[ 0.8910,  2.0469, -0.8444,  1.2223, -0.6239,  0.9987, -0.0146,\n","          -0.7522,  0.1654, -1.4761,  0.0176,  0.2785, -1.8793, -0.3907,\n","           0.8218, -0.4609]],\n","\n","        [[ 1.6259,  0.7138,  1.0001,  0.1423, -0.5523, -0.3360, -1.2479,\n","           1.2604, -1.2699,  0.0720, -0.6216,  0.9255, -0.3482,  1.0841,\n","          -1.9686, -0.4795]]], grad_fn=<NativeLayerNormBackward0>)\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[ 1.1035, -0.8290, -0.0885, -0.1914, -0.1199,  0.8720, -0.3894,\n","          -1.4592, -0.0203, -1.6063,  0.5217, -0.1248, -0.6993, -0.7543,\n","          -0.1203,  0.2733]],\n","\n","        [[ 0.0398, -0.9035, -0.2130,  0.4059,  0.2935, -0.0724, -1.0229,\n","          -0.5496,  0.8903, -0.8247, -0.9796,  0.5409, -1.3512,  0.8544,\n","           0.0031,  0.0314]],\n","\n","        [[-0.0482, -1.3927,  0.3418,  0.4482,  0.2586,  1.5715,  0.7219,\n","           0.1536, -1.1503, -0.9000, -0.2168, -0.6097, -0.8467,  0.1502,\n","          -0.7738,  0.8109]],\n","\n","        [[ 1.0953, -0.3946,  0.1049, -0.3839,  0.1203, -0.7053, -0.8756,\n","          -0.7326,  0.5444, -0.5186,  0.0184, -0.1431,  0.4677, -1.1472,\n","          -0.0669,  1.1585]]], grad_fn=<SelectBackward0>)\n","\n","key matrix :-  tensor([[[-0.6337,  0.4840, -0.8896,  0.8596, -0.1976, -1.2982, -0.2527,\n","           0.6885,  1.0370,  0.8539,  0.8168, -0.9077,  0.2089, -0.2613,\n","          -0.0927, -0.0693]],\n","\n","        [[-0.4705,  0.6798,  0.0238,  1.0980,  0.0602, -0.7898,  0.3233,\n","          -0.7603,  0.8717, -0.5767,  1.0535,  0.1226,  1.3968, -0.9623,\n","           1.2267,  0.1423]],\n","\n","        [[-0.2556,  0.2990, -0.9382,  1.0964, -0.3094, -0.8896,  0.9999,\n","          -1.0754,  0.7991, -0.2584,  0.2761,  0.1726,  0.7949, -0.1626,\n","           0.3344,  0.2674]],\n","\n","        [[-1.0865,  0.8603,  0.6244,  0.5369, -0.2122,  0.3337, -0.5033,\n","          -0.3542,  1.8765,  0.1435,  1.0252,  0.7959,  0.2711, -1.1477,\n","           0.4108,  0.1170]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[-1.2182,  0.3836, -0.1322, -0.9554,  0.8654, -1.2112, -0.6575,\n","           0.2516, -0.0950, -1.4153, -0.1284,  0.5638, -0.3141, -0.1734,\n","           1.2666, -0.2924]],\n","\n","        [[-0.0627, -0.0788, -1.0953, -1.7505,  0.0047, -0.6611,  0.6803,\n","           0.7156, -0.6196, -0.2924, -0.7830,  0.0273, -0.7326,  0.1281,\n","           0.7943, -0.3551]],\n","\n","        [[ 0.6258, -0.1710, -0.9977, -0.6461, -0.8907, -0.6342,  0.3602,\n","          -0.4221,  0.7373, -0.6742,  0.1639, -0.0821, -1.1925, -0.5093,\n","           1.1378, -1.7853]],\n","\n","        [[-0.6672,  1.0903, -0.3333, -0.7208,  0.2069,  0.1347, -1.8261,\n","           1.2324, -0.9645, -1.3206, -0.4324, -0.1198,  0.3928, -1.2805,\n","          -0.1732,  1.2856]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[ 1.1035, -0.8290, -0.0885, -0.1914],\n","          [ 0.0398, -0.9035, -0.2130,  0.4059],\n","          [-0.0482, -1.3927,  0.3418,  0.4482],\n","          [ 1.0953, -0.3946,  0.1049, -0.3839]],\n","\n","         [[-0.1199,  0.8720, -0.3894, -1.4592],\n","          [ 0.2935, -0.0724, -1.0229, -0.5496],\n","          [ 0.2586,  1.5715,  0.7219,  0.1536],\n","          [ 0.1203, -0.7053, -0.8756, -0.7326]],\n","\n","         [[-0.0203, -1.6063,  0.5217, -0.1248],\n","          [ 0.8903, -0.8247, -0.9796,  0.5409],\n","          [-1.1503, -0.9000, -0.2168, -0.6097],\n","          [ 0.5444, -0.5186,  0.0184, -0.1431]],\n","\n","         [[-0.6993, -0.7543, -0.1203,  0.2733],\n","          [-1.3512,  0.8544,  0.0031,  0.0314],\n","          [-0.8467,  0.1502, -0.7738,  0.8109],\n","          [ 0.4677, -1.1472, -0.0669,  1.1585]]]], grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[-0.6337,  0.4840, -0.8896,  0.8596],\n","          [-0.4705,  0.6798,  0.0238,  1.0980],\n","          [-0.2556,  0.2990, -0.9382,  1.0964],\n","          [-1.0865,  0.8603,  0.6244,  0.5369]],\n","\n","         [[-0.1976, -1.2982, -0.2527,  0.6885],\n","          [ 0.0602, -0.7898,  0.3233, -0.7603],\n","          [-0.3094, -0.8896,  0.9999, -1.0754],\n","          [-0.2122,  0.3337, -0.5033, -0.3542]],\n","\n","         [[ 1.0370,  0.8539,  0.8168, -0.9077],\n","          [ 0.8717, -0.5767,  1.0535,  0.1226],\n","          [ 0.7991, -0.2584,  0.2761,  0.1726],\n","          [ 1.8765,  0.1435,  1.0252,  0.7959]],\n","\n","         [[ 0.2089, -0.2613, -0.0927, -0.0693],\n","          [ 1.3968, -0.9623,  1.2267,  0.1423],\n","          [ 0.7949, -0.1626,  0.3344,  0.2674],\n","          [ 0.2711, -1.1477,  0.4108,  0.1170]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[-1.2182,  0.3836, -0.1322, -0.9554],\n","          [-0.0627, -0.0788, -1.0953, -1.7505],\n","          [ 0.6258, -0.1710, -0.9977, -0.6461],\n","          [-0.6672,  1.0903, -0.3333, -0.7208]],\n","\n","         [[ 0.8654, -1.2112, -0.6575,  0.2516],\n","          [ 0.0047, -0.6611,  0.6803,  0.7156],\n","          [-0.8907, -0.6342,  0.3602, -0.4221],\n","          [ 0.2069,  0.1347, -1.8261,  1.2324]],\n","\n","         [[-0.0950, -1.4153, -0.1284,  0.5638],\n","          [-0.6196, -0.2924, -0.7830,  0.0273],\n","          [ 0.7373, -0.6742,  0.1639, -0.0821],\n","          [-0.9645, -1.3206, -0.4324, -0.1198]],\n","\n","         [[-0.3141, -0.1734,  1.2666, -0.2924],\n","          [-0.7326,  0.1281,  0.7943, -0.3551],\n","          [-1.1925, -0.5093,  1.1378, -1.7853],\n","          [ 0.3928, -1.2805, -0.1732,  1.2856]]]], grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[-0.2289,  0.2021, -0.6894, -1.0065],\n","          [-0.2691,  0.2283, -0.6669, -1.0045],\n","          [-0.2801,  0.2673, -0.6673, -1.0193],\n","          [-0.2632,  0.2442, -0.6769, -1.0203]],\n","\n","         [[-0.0998, -0.3989, -0.4642,  0.5527],\n","          [ 0.0973, -0.5230, -0.4995,  0.5624],\n","          [-0.0211, -0.4091, -0.5753,  0.5816],\n","          [ 0.0542, -0.6093, -0.3086,  0.4478]],\n","\n","         [[-0.2804, -0.7529, -0.3832,  0.0368],\n","          [-0.2638, -0.8914, -0.2998,  0.0097],\n","          [-0.0974, -0.7852, -0.2875,  0.0964],\n","          [-0.2797, -0.9074, -0.3198,  0.0719]],\n","\n","         [[-0.3352, -0.5576,  0.6514, -0.0664],\n","          [-0.4231, -0.4943,  0.8229, -0.3055],\n","          [-0.4010, -0.5258,  0.7781, -0.2543],\n","          [-0.4299, -0.4661,  0.6559, -0.1581]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","After self-attention block: tensor([[[ 0.2127, -0.0069, -0.0368,  0.2845,  0.3859, -0.2359, -0.4430,\n","          -0.1919,  0.0931,  0.5296,  0.1898, -0.0150, -0.3800,  0.0614,\n","          -0.3258, -0.1202]],\n","\n","        [[ 0.3132,  0.0814,  0.1239,  0.3478,  0.2132, -0.2092, -0.5329,\n","          -0.0718, -0.0756,  0.6930,  0.1345, -0.0982, -0.4718,  0.0931,\n","          -0.2479, -0.2196]],\n","\n","        [[ 0.2892, -0.0516,  0.0377,  0.3481,  0.3244, -0.1718, -0.4476,\n","          -0.1127, -0.0553,  0.6348,  0.1569, -0.1465, -0.4833,  0.1305,\n","          -0.3906, -0.1629]],\n","\n","        [[ 0.2309,  0.1101, -0.0291,  0.2476,  0.3503, -0.0894, -0.5672,\n","          -0.0921, -0.0484,  0.6783,  0.1880, -0.0268, -0.4910, -0.0047,\n","          -0.1331, -0.2743]]], grad_fn=<ViewBackward0>)\n","After adding self-attention output to x: tensor([[[ 7.0201e-01,  6.6679e-01, -7.8545e-01,  1.9751e+00, -3.3305e-01,\n","           3.4594e-01, -1.4353e-01, -1.1378e+00, -1.3171e+00,  6.6507e-01,\n","          -8.0302e-01,  9.9655e-03, -1.3215e+00,  2.4545e+00, -9.7881e-01,\n","           2.3474e-03]],\n","\n","        [[ 2.0553e+00,  6.0064e-01, -1.2960e+00,  7.8118e-01, -2.2753e+00,\n","           9.1563e-01, -1.3055e+00,  1.9187e-01, -1.4150e-01,  5.8146e-01,\n","           9.0062e-01,  4.8829e-01, -1.5696e+00,  2.4200e-01, -3.1021e-01,\n","           2.1423e-01]],\n","\n","        [[ 1.1802e+00,  1.9953e+00, -8.0670e-01,  1.5704e+00, -2.9951e-01,\n","           8.2686e-01, -4.6221e-01, -8.6490e-01,  1.1006e-01, -8.4132e-01,\n","           1.7448e-01,  1.3198e-01, -2.3626e+00, -2.6016e-01,  4.3123e-01,\n","          -6.2378e-01]],\n","\n","        [[ 1.8568e+00,  8.2389e-01,  9.7104e-01,  3.8982e-01, -2.0204e-01,\n","          -4.2541e-01, -1.8151e+00,  1.1683e+00, -1.3183e+00,  7.5030e-01,\n","          -4.3361e-01,  8.9870e-01, -8.3915e-01,  1.0793e+00, -2.1017e+00,\n","          -7.5379e-01]]], grad_fn=<AddBackward0>)\n","After norm1: tensor([[[ 6.5185e-01,  6.1914e-01, -7.2951e-01,  1.8341e+00, -3.0939e-01,\n","           3.2118e-01, -1.3338e-01, -1.0568e+00, -1.2232e+00,  6.1754e-01,\n","          -7.4583e-01,  9.1667e-03, -1.2273e+00,  2.2793e+00, -9.0908e-01,\n","           2.0920e-03]],\n","\n","        [[ 1.8968e+00,  5.5131e-01, -1.2029e+00,  7.1830e-01, -2.1087e+00,\n","           8.4265e-01, -1.2117e+00,  1.7323e-01, -1.3510e-01,  5.3357e-01,\n","           8.2877e-01,  4.4740e-01, -1.4559e+00,  2.1960e-01, -2.9114e-01,\n","           1.9392e-01]],\n","\n","        [[ 1.1438e+00,  1.9295e+00, -7.7158e-01,  1.5199e+00, -2.8266e-01,\n","           8.0314e-01, -4.3950e-01, -8.2768e-01,  1.1216e-01, -8.0495e-01,\n","           1.7426e-01,  1.3329e-01, -2.2714e+00, -2.4472e-01,  4.2176e-01,\n","          -5.9525e-01]],\n","\n","        [[ 1.6476e+00,  7.2954e-01,  8.6033e-01,  3.4374e-01, -1.8230e-01,\n","          -3.8084e-01, -1.6160e+00,  1.0357e+00, -1.1745e+00,  6.6414e-01,\n","          -3.8813e-01,  7.9604e-01, -7.4857e-01,  9.5660e-01, -1.8707e+00,\n","          -6.7270e-01]]], grad_fn=<NativeLayerNormBackward0>)\n","After feed-forward block: tensor([[[ 9.5851e-01,  4.2865e-01, -5.4520e-01, -8.7168e-01,  9.7738e-01,\n","           8.0358e-01, -1.4423e-01, -8.5441e-01,  6.8198e-02, -3.2936e-01,\n","           8.0885e-01,  8.2844e-01,  3.0064e-01,  7.0465e-01, -9.7592e-01,\n","           8.2061e-01]],\n","\n","        [[ 1.0223e+00,  3.3414e-01, -1.2396e-01,  7.4243e-01,  4.9031e-01,\n","          -2.4007e-02, -1.5056e-01,  7.0397e-01,  5.0189e-02, -1.0061e+00,\n","           1.6575e-01, -3.4665e-01, -1.2095e+00,  9.2222e-01,  1.1921e-01,\n","           1.3745e-03]],\n","\n","        [[ 4.5234e-01,  4.6861e-01, -4.0531e-01, -3.2366e-02,  1.0324e+00,\n","           1.0540e-01,  2.8976e-02, -7.9231e-02,  3.0124e-01, -2.7868e-01,\n","           1.3747e-01,  5.2829e-01, -4.4701e-01,  3.0866e-01, -4.4803e-01,\n","          -2.3604e-02]],\n","\n","        [[ 2.1329e+00, -2.2972e-01,  1.6101e-01, -5.5966e-01, -1.8569e-01,\n","           8.0755e-02,  6.9445e-01,  4.1688e-01,  1.5174e-01, -1.3937e+00,\n","           1.0557e+00,  2.4832e-01, -1.4710e+00,  5.4835e-01, -1.3747e+00,\n","           1.7559e+00]]], grad_fn=<ViewBackward0>)\n","After adding feed-forward output to x: tensor([[[ 1.6104,  1.0478, -1.2747,  0.9625,  0.6680,  1.1248, -0.2776,\n","          -1.9112, -1.1550,  0.2882,  0.0630,  0.8376, -0.9267,  2.9840,\n","          -1.8850,  0.8227]],\n","\n","        [[ 2.9191,  0.8855, -1.3269,  1.4607, -1.6184,  0.8186, -1.3623,\n","           0.8772, -0.0849, -0.4726,  0.9945,  0.1007, -2.6654,  1.1418,\n","          -0.1719,  0.1953]],\n","\n","        [[ 1.5961,  2.3981, -1.1769,  1.4876,  0.7497,  0.9085, -0.4105,\n","          -0.9069,  0.4134, -1.0836,  0.3117,  0.6616, -2.7184,  0.0639,\n","          -0.0263, -0.6189]],\n","\n","        [[ 3.7805,  0.4998,  1.0213, -0.2159, -0.3680, -0.3001, -0.9216,\n","           1.4525, -1.0227, -0.7296,  0.6676,  1.0444, -2.2196,  1.5050,\n","          -3.2454,  1.0831]]], grad_fn=<AddBackward0>)\n","After norm2: tensor([[[ 1.0914,  0.6603, -1.1195,  0.5949,  0.3692,  0.7193, -0.3554,\n","          -1.6073, -1.0278,  0.0782, -0.0944,  0.4992, -0.8528,  2.1441,\n","          -1.5872,  0.4878]],\n","\n","        [[ 2.1028,  0.5828, -1.0707,  1.0128, -1.2886,  0.5329, -1.0972,\n","           0.5766, -0.1425, -0.4322,  0.6643, -0.0037, -2.0712,  0.7744,\n","          -0.2075,  0.0670]],\n","\n","        [[ 1.2251,  1.8832, -1.0503,  1.1361,  0.5306,  0.6609, -0.4214,\n","          -0.8288,  0.2546, -0.9738,  0.1712,  0.4583, -2.3152, -0.0321,\n","          -0.1061, -0.5924]],\n","\n","        [[ 2.2944,  0.2342,  0.5617, -0.2153, -0.3108, -0.2682, -0.6585,\n","           0.8325, -0.7220, -0.5379,  0.3395,  0.5761, -1.4736,  0.8654,\n","          -2.1178,  0.6005]]], grad_fn=<NativeLayerNormBackward0>)\n","Apply attention or feedforward layers.Normalize the output.\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[ 0.3176, -1.1368,  0.3084, -0.0130, -0.1129, -0.0069, -1.6328,\n","           0.4974, -0.6126,  1.7699, -0.3587,  1.0307,  0.4321, -1.3295,\n","          -0.1868,  0.6173]],\n","\n","        [[-0.2553, -0.7214, -0.9945, -0.4704,  0.5905, -1.2389,  0.2282,\n","           0.8913,  1.2047,  0.3954,  0.9789,  0.1859, -1.0038, -0.2742,\n","           0.0754, -0.4191]],\n","\n","        [[ 0.3178,  0.1473, -0.0350, -0.1405,  0.0385, -1.0608, -0.7965,\n","           0.2419,  0.4368,  0.2191, -0.0481,  1.3612, -0.3872, -0.3117,\n","           0.2035,  0.3925]],\n","\n","        [[-1.7459, -0.5454, -1.1420,  0.5576,  0.5233, -1.5746, -0.5162,\n","          -0.4611, -0.1574,  0.1187,  0.4268,  1.3912, -1.5547, -1.4246,\n","           0.8551,  0.0811]]], grad_fn=<SelectBackward0>)\n","\n","key matrix :-  tensor([[[-0.2193, -0.5837, -1.0835,  0.2208,  0.3048, -1.0133,  1.0321,\n","          -0.2144, -0.5868,  0.0285,  0.2824,  0.1164,  0.7213,  1.1734,\n","           0.1988, -0.8131]],\n","\n","        [[-0.2567,  0.7553, -1.4254, -0.5359, -1.1070,  0.1800,  0.4925,\n","          -0.8620,  0.9776,  0.2585, -0.1611,  1.4639,  0.8481, -0.3776,\n","           1.2645,  0.7462]],\n","\n","        [[-0.5738,  0.2403, -1.5936,  1.5045,  0.3419, -0.1810, -0.2571,\n","          -0.3787, -0.7214, -0.2292, -0.3831,  0.6826,  0.0058,  0.7510,\n","           0.7880, -0.0931]],\n","\n","        [[-0.3725, -0.3239, -1.1651,  1.3124, -1.2964, -0.6673,  0.9517,\n","          -0.5908,  0.0941,  0.8827, -0.7832, -0.6341,  0.5970,  0.5257,\n","           0.1523, -1.7002]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[ 8.1363e-01, -1.1846e+00,  3.9668e-01, -6.1135e-01,  1.7841e+00,\n","          -6.5266e-01,  7.2256e-01, -5.7461e-02,  3.4997e-01,  1.7250e-01,\n","          -1.2689e-01, -1.0235e+00,  2.6455e-02,  8.4173e-01,  1.0113e+00,\n","           2.2698e+00]],\n","\n","        [[ 6.0762e-01, -1.8152e+00, -1.5807e-01, -4.2683e-01,  1.5784e-01,\n","           6.0005e-01, -1.1977e+00,  1.5466e+00,  1.5545e-01, -3.5194e-01,\n","          -2.8855e-01, -3.2984e-01, -3.6546e-02,  1.1753e+00,  1.6788e+00,\n","           2.9423e-01]],\n","\n","        [[-3.4261e-02, -3.4991e-01, -9.9588e-01, -6.0269e-01,  4.4100e-01,\n","           4.6418e-01, -7.8054e-01, -1.9438e-01,  4.5676e-02,  7.2861e-01,\n","          -1.5582e-01, -2.5398e-01,  9.1939e-02, -5.6900e-01,  2.4899e-01,\n","           1.2655e+00]],\n","\n","        [[ 2.6563e-04, -1.0985e+00, -4.4167e-01,  8.4690e-01,  1.1539e+00,\n","           6.2293e-01,  8.0650e-01,  4.9947e-02, -8.7031e-01, -1.1300e+00,\n","           1.3467e+00, -6.5219e-01, -1.2030e+00,  1.8730e+00,  9.4547e-01,\n","           1.4448e+00]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[ 0.3176, -1.1368,  0.3084, -0.0130],\n","          [-0.2553, -0.7214, -0.9945, -0.4704],\n","          [ 0.3178,  0.1473, -0.0350, -0.1405],\n","          [-1.7459, -0.5454, -1.1420,  0.5576]],\n","\n","         [[-0.1129, -0.0069, -1.6328,  0.4974],\n","          [ 0.5905, -1.2389,  0.2282,  0.8913],\n","          [ 0.0385, -1.0608, -0.7965,  0.2419],\n","          [ 0.5233, -1.5746, -0.5162, -0.4611]],\n","\n","         [[-0.6126,  1.7699, -0.3587,  1.0307],\n","          [ 1.2047,  0.3954,  0.9789,  0.1859],\n","          [ 0.4368,  0.2191, -0.0481,  1.3612],\n","          [-0.1574,  0.1187,  0.4268,  1.3912]],\n","\n","         [[ 0.4321, -1.3295, -0.1868,  0.6173],\n","          [-1.0038, -0.2742,  0.0754, -0.4191],\n","          [-0.3872, -0.3117,  0.2035,  0.3925],\n","          [-1.5547, -1.4246,  0.8551,  0.0811]]]], grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[-0.2193, -0.5837, -1.0835,  0.2208],\n","          [-0.2567,  0.7553, -1.4254, -0.5359],\n","          [-0.5738,  0.2403, -1.5936,  1.5045],\n","          [-0.3725, -0.3239, -1.1651,  1.3124]],\n","\n","         [[ 0.3048, -1.0133,  1.0321, -0.2144],\n","          [-1.1070,  0.1800,  0.4925, -0.8620],\n","          [ 0.3419, -0.1810, -0.2571, -0.3787],\n","          [-1.2964, -0.6673,  0.9517, -0.5908]],\n","\n","         [[-0.5868,  0.0285,  0.2824,  0.1164],\n","          [ 0.9776,  0.2585, -0.1611,  1.4639],\n","          [-0.7214, -0.2292, -0.3831,  0.6826],\n","          [ 0.0941,  0.8827, -0.7832, -0.6341]],\n","\n","         [[ 0.7213,  1.1734,  0.1988, -0.8131],\n","          [ 0.8481, -0.3776,  1.2645,  0.7462],\n","          [ 0.0058,  0.7510,  0.7880, -0.0931],\n","          [ 0.5970,  0.5257,  0.1523, -1.7002]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[ 8.1363e-01, -1.1846e+00,  3.9668e-01, -6.1135e-01],\n","          [ 6.0762e-01, -1.8152e+00, -1.5807e-01, -4.2683e-01],\n","          [-3.4261e-02, -3.4991e-01, -9.9588e-01, -6.0269e-01],\n","          [ 2.6563e-04, -1.0985e+00, -4.4167e-01,  8.4690e-01]],\n","\n","         [[ 1.7841e+00, -6.5266e-01,  7.2256e-01, -5.7461e-02],\n","          [ 1.5784e-01,  6.0005e-01, -1.1977e+00,  1.5466e+00],\n","          [ 4.4100e-01,  4.6418e-01, -7.8054e-01, -1.9438e-01],\n","          [ 1.1539e+00,  6.2293e-01,  8.0650e-01,  4.9947e-02]],\n","\n","         [[ 3.4997e-01,  1.7250e-01, -1.2689e-01, -1.0235e+00],\n","          [ 1.5545e-01, -3.5194e-01, -2.8855e-01, -3.2984e-01],\n","          [ 4.5676e-02,  7.2861e-01, -1.5582e-01, -2.5398e-01],\n","          [-8.7031e-01, -1.1300e+00,  1.3467e+00, -6.5219e-01]],\n","\n","         [[ 2.6455e-02,  8.4173e-01,  1.0113e+00,  2.2698e+00],\n","          [-3.6546e-02,  1.1753e+00,  1.6788e+00,  2.9423e-01],\n","          [ 9.1939e-02, -5.6900e-01,  2.4899e-01,  1.2655e+00],\n","          [-1.2030e+00,  1.8730e+00,  9.4547e-01,  1.4448e+00]]]],\n","       grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[ 0.8136, -1.1846,  0.3967, -0.6114],\n","          [ 0.7173, -1.4794,  0.1373, -0.5251],\n","          [ 0.4819, -1.1720, -0.2296, -0.5392],\n","          [ 0.2210, -0.9160, -0.4636, -0.1673]],\n","\n","         [[ 1.7841, -0.6527,  0.7226, -0.0575],\n","          [ 1.4889, -0.4253,  0.3740,  0.2337],\n","          [ 0.8851,  0.0726, -0.3060,  0.2471],\n","          [ 1.0529,  0.0985,  0.0552,  0.1420]],\n","\n","         [[ 0.3500,  0.1725, -0.1269, -1.0235],\n","          [ 0.2118, -0.1999, -0.2417, -0.5309],\n","          [ 0.1620, -0.0140, -0.2310, -0.4267],\n","          [ 0.0666, -0.0331, -0.0605, -0.4847]],\n","\n","         [[ 0.0265,  0.8417,  1.0113,  2.2698],\n","          [-0.0029,  0.9971,  1.3221,  1.3499],\n","          [ 0.0218,  0.5018,  1.0368,  1.0778],\n","          [-0.1777,  0.7293,  1.0672,  0.9605]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","x (tgt)=  tensor([[[ 0.8854,  1.1824,  0.7864,  0.9421,  0.5667,  0.2902, -0.4875,\n","           1.0501,  0.6084,  2.6309, -0.0847,  2.0844,  0.9478,  0.3234,\n","          -0.5730,  0.6697]],\n","\n","        [[-0.8032, -0.1209,  0.1956,  0.2185, -1.7899,  0.7384, -0.4403,\n","           3.1848, -0.4801, -0.2872,  0.7389,  1.0339, -0.3123,  0.7458,\n","          -1.2055,  0.0458]],\n","\n","        [[ 0.0247, -0.0641, -0.7602,  0.5925,  0.9624,  0.8574,  0.1527,\n","           0.9612,  0.9446, -0.5824,  0.9871,  2.1457, -0.1418,  0.7237,\n","          -0.1932,  1.7768]],\n","\n","        [[-0.2184,  1.1663,  2.1442,  2.7046,  0.3459,  1.6425, -0.2040,\n","           1.6854, -0.1397, -0.1808, -1.2829,  1.4485, -0.5907,  1.8541,\n","          -0.4901,  0.6405]]], grad_fn=<AddBackward0>)\n","after attention=  tensor([[[-0.3056, -1.1210,  0.1713,  1.1366,  0.0546,  0.3289, -0.5426,\n","           0.9582,  0.8728,  0.8922, -0.2171, -0.7690,  0.0794, -1.1795,\n","           0.4314, -2.4202]],\n","\n","        [[-0.0700, -0.4967, -0.2370,  0.8734,  0.0216,  0.4894, -0.4589,\n","           0.8478,  0.9642,  0.4137, -0.0977, -0.8155, -0.0099, -0.9528,\n","          -0.0800, -2.1234]],\n","\n","        [[-0.3461, -0.4599, -0.4122,  0.4316, -0.2019,  0.3248, -0.0795,\n","           0.4085,  0.8973,  0.5808, -0.1066, -0.5336, -0.0257, -0.4541,\n","          -0.3013, -1.3768]],\n","\n","        [[-0.0112, -0.5438, -0.4369,  0.4788, -0.0324,  0.1009, -0.1067,\n","           0.3386,  0.6747,  0.7608, -0.0929, -0.1838, -0.3497, -0.6725,\n","          -0.3098, -1.0956]]], grad_fn=<ViewBackward0>)\n","after norm1 = \n","tensor([[[-0.0447, -0.4494,  0.2503,  1.1255, -0.0124, -0.0140, -1.3016,\n","           1.0706,  0.6591,  2.2533, -0.7330,  0.5296,  0.3046, -1.1658,\n","          -0.6079, -1.8641]],\n","\n","        [[-0.6166, -0.4326, -0.0177,  0.7983, -1.2610,  0.8962, -0.6353,\n","           2.9157,  0.3607,  0.1032,  0.4738,  0.1694, -0.2199, -0.1369,\n","          -0.9135, -1.4838]],\n","\n","        [[-0.8912, -1.1345, -1.9129,  0.7244,  0.4079,  0.9142, -0.4173,\n","           1.1394,  1.7064, -0.5071,  0.5521,  1.4304, -0.7063, -0.1816,\n","          -1.0990, -0.0250]],\n","\n","        [[-0.6637,  0.0478,  0.9535,  2.1861, -0.2102,  0.9837, -0.7313,\n","           1.2180, -0.0252,  0.0123, -1.6207,  0.5840, -1.2572,  0.5146,\n","          -1.1398, -0.8519]]], grad_fn=<NativeLayerNormBackward0>)\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[ 1.7767, -0.2418, -0.2308,  0.5786,  0.4684,  0.4363,  0.4515,\n","           0.6068, -0.3450,  0.5240,  1.3942, -1.2978,  0.3934,  1.9971,\n","          -0.0349,  0.0635]],\n","\n","        [[ 1.4235,  0.3432, -0.0458,  1.0765, -0.2108,  0.1751, -0.1897,\n","           0.8528,  0.0095,  0.0221,  0.7111, -0.9511, -0.0159,  1.3848,\n","           1.0502, -0.3414]],\n","\n","        [[-0.0826,  0.1198,  0.3023,  2.2164, -0.6720, -0.3389, -0.3846,\n","           0.1488,  0.2696, -0.0308,  1.6402, -1.1025, -0.5374, -0.0804,\n","           0.3957,  0.2037]],\n","\n","        [[ 0.7956, -0.5747,  0.3332,  1.1389, -0.2890, -0.4912,  0.1224,\n","           1.0680,  0.2141,  0.4608, -0.0238, -0.6043,  0.3818,  1.5359,\n","           0.4260,  0.9293]]], grad_fn=<ViewBackward0>)\n","\n","key matrix :-  tensor([[[-0.5683,  0.5526,  1.8797, -0.0390, -1.1555,  0.6054, -1.2049,\n","          -0.6618, -0.7020, -1.0353,  0.6963, -0.4222, -0.3018,  0.1346,\n","           0.7480, -0.6850]],\n","\n","        [[-0.5837, -0.7209,  0.7567, -0.6584, -1.0321,  0.6136, -0.3782,\n","           0.4042,  0.4988,  0.3934,  0.5759,  0.2909, -1.1517, -0.3398,\n","           0.4572, -0.9528]],\n","\n","        [[-0.7392, -0.4514,  0.6723, -0.6893, -1.3262,  0.2071, -0.2459,\n","          -0.3289,  0.3931,  0.0321,  0.4806,  0.4141, -0.2356, -0.7088,\n","          -0.2461,  0.1491]],\n","\n","        [[ 0.4167, -0.4246,  1.4235, -0.4369, -0.6567,  0.4626,  0.0621,\n","           0.1336,  0.1379, -0.0900, -0.3644, -0.7326, -1.2655, -0.4718,\n","           0.8877, -1.6053]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[-0.5589, -0.0286,  0.2069, -0.7922,  0.6605,  0.7171,  0.0569,\n","           0.0285, -0.5483,  1.1559,  0.5180, -0.0207,  1.7518,  0.4668,\n","          -0.8743, -0.4177]],\n","\n","        [[-1.4642, -1.0460, -0.4508,  0.1007,  0.0471,  1.0841,  0.0140,\n","           0.4299, -0.9654,  0.4517,  0.0774, -1.0476,  0.9084, -0.4952,\n","           0.3278, -0.0330]],\n","\n","        [[-0.8815, -0.3710,  0.9188, -0.0989,  0.6470,  1.4464,  0.5698,\n","          -0.3237, -1.0699,  0.9270,  0.0511, -0.3763,  1.2063, -1.3311,\n","          -0.3342,  0.1697]],\n","\n","        [[-1.4960,  0.4016, -0.2091, -0.5910,  0.3056,  0.4931, -0.8481,\n","           0.3787, -1.0921,  0.8650, -0.4011, -0.1086,  0.5782,  0.6896,\n","           0.5965, -0.5569]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[ 1.7767, -0.2418, -0.2308,  0.5786],\n","          [ 1.4235,  0.3432, -0.0458,  1.0765],\n","          [-0.0826,  0.1198,  0.3023,  2.2164],\n","          [ 0.7956, -0.5747,  0.3332,  1.1389]],\n","\n","         [[ 0.4684,  0.4363,  0.4515,  0.6068],\n","          [-0.2108,  0.1751, -0.1897,  0.8528],\n","          [-0.6720, -0.3389, -0.3846,  0.1488],\n","          [-0.2890, -0.4912,  0.1224,  1.0680]],\n","\n","         [[-0.3450,  0.5240,  1.3942, -1.2978],\n","          [ 0.0095,  0.0221,  0.7111, -0.9511],\n","          [ 0.2696, -0.0308,  1.6402, -1.1025],\n","          [ 0.2141,  0.4608, -0.0238, -0.6043]],\n","\n","         [[ 0.3934,  1.9971, -0.0349,  0.0635],\n","          [-0.0159,  1.3848,  1.0502, -0.3414],\n","          [-0.5374, -0.0804,  0.3957,  0.2037],\n","          [ 0.3818,  1.5359,  0.4260,  0.9293]]]], grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[-0.5683,  0.5526,  1.8797, -0.0390],\n","          [-0.5837, -0.7209,  0.7567, -0.6584],\n","          [-0.7392, -0.4514,  0.6723, -0.6893],\n","          [ 0.4167, -0.4246,  1.4235, -0.4369]],\n","\n","         [[-1.1555,  0.6054, -1.2049, -0.6618],\n","          [-1.0321,  0.6136, -0.3782,  0.4042],\n","          [-1.3262,  0.2071, -0.2459, -0.3289],\n","          [-0.6567,  0.4626,  0.0621,  0.1336]],\n","\n","         [[-0.7020, -1.0353,  0.6963, -0.4222],\n","          [ 0.4988,  0.3934,  0.5759,  0.2909],\n","          [ 0.3931,  0.0321,  0.4806,  0.4141],\n","          [ 0.1379, -0.0900, -0.3644, -0.7326]],\n","\n","         [[-0.3018,  0.1346,  0.7480, -0.6850],\n","          [-1.1517, -0.3398,  0.4572, -0.9528],\n","          [-0.2356, -0.7088, -0.2461,  0.1491],\n","          [-1.2655, -0.4718,  0.8877, -1.6053]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[-0.5589, -0.0286,  0.2069, -0.7922],\n","          [-1.4642, -1.0460, -0.4508,  0.1007],\n","          [-0.8815, -0.3710,  0.9188, -0.0989],\n","          [-1.4960,  0.4016, -0.2091, -0.5910]],\n","\n","         [[ 0.6605,  0.7171,  0.0569,  0.0285],\n","          [ 0.0471,  1.0841,  0.0140,  0.4299],\n","          [ 0.6470,  1.4464,  0.5698, -0.3237],\n","          [ 0.3056,  0.4931, -0.8481,  0.3787]],\n","\n","         [[-0.5483,  1.1559,  0.5180, -0.0207],\n","          [-0.9654,  0.4517,  0.0774, -1.0476],\n","          [-1.0699,  0.9270,  0.0511, -0.3763],\n","          [-1.0921,  0.8650, -0.4011, -0.1086]],\n","\n","         [[ 1.7518,  0.4668, -0.8743, -0.4177],\n","          [ 0.9084, -0.4952,  0.3278, -0.0330],\n","          [ 1.2063, -1.3311, -0.3342,  0.1697],\n","          [ 0.5782,  0.6896,  0.5965, -0.5569]]]], grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[-1.2189, -0.0896,  0.0057, -0.4090],\n","          [-1.1297, -0.0801,  0.0423, -0.4576],\n","          [-0.9853, -0.1571,  0.1157, -0.4789],\n","          [-1.1458, -0.1460,  0.0437, -0.4097]],\n","\n","         [[ 0.3617,  0.9075, -0.1378,  0.1861],\n","          [ 0.3781,  0.9372, -0.0696,  0.1625],\n","          [ 0.4333,  0.9601,  0.0024,  0.1034],\n","          [ 0.3693,  0.9491, -0.0774,  0.1627]],\n","\n","         [[-0.8689,  0.8796,  0.1179, -0.3521],\n","          [-0.8861,  0.8784,  0.0895, -0.3434],\n","          [-0.8657,  0.8782,  0.1342, -0.3657],\n","          [-0.9433,  0.8310,  0.0137, -0.3959]],\n","\n","         [[ 1.2535, -0.0407, -0.2454, -0.2486],\n","          [ 1.1441,  0.1217, -0.0935, -0.3084],\n","          [ 1.0710, -0.1083, -0.0118, -0.2297],\n","          [ 1.2787, -0.1517, -0.2893, -0.2111]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","output after attention and normailsation(2) = \n","tensor([[[ 0.4740, -1.0972,  0.7448,  0.8050, -0.4046,  0.0354, -0.6120,\n","           1.4899,  0.5895,  1.4987, -0.7374,  1.5588, -0.6076, -1.2439,\n","          -1.1546, -1.3388]],\n","\n","        [[-0.0197, -0.9329,  0.5331,  0.4924, -1.3389,  0.7359, -0.0789,\n","           2.7381,  0.2258, -0.2834,  0.1416,  1.1626, -0.8917, -0.3249,\n","          -1.2040, -0.9549]],\n","\n","        [[-0.1563, -1.3810, -0.9988,  0.4518, -0.1037,  0.8171,  0.0809,\n","           1.4937,  1.2666, -0.8199,  0.2550,  1.9726, -1.2342, -0.3532,\n","          -1.4315,  0.1409]],\n","\n","        [[ 0.0228, -0.5905,  1.1380,  1.5393, -0.4946,  0.7446, -0.1309,\n","           1.4982,  0.0093, -0.3297, -1.3246,  1.4104, -1.6438,  0.1288,\n","          -1.4353, -0.5421]]], grad_fn=<NativeLayerNormBackward0>)\n"," after feed forward and normailsation(3) = \n","tensor([[[ 0.4540, -1.3520,  0.7491, -0.0715, -0.7477,  0.2000, -0.8822,\n","           1.0342,  0.6194,  1.4088, -0.6975,  2.0692,  0.2639, -1.5740,\n","          -1.0395, -0.4342]],\n","\n","        [[-0.1171, -1.3384,  0.3916, -0.4699, -1.4920,  0.3896, -0.4171,\n","           2.4554,  0.0343,  0.2821,  0.3009,  1.8131,  0.1982, -0.6167,\n","          -1.1325, -0.2815]],\n","\n","        [[-0.5830, -1.1824, -0.1906, -0.7489, -0.2649,  0.4737, -0.1493,\n","           1.8754,  0.7224, -0.4005,  0.2510,  2.3580, -0.0194, -0.8357,\n","          -1.6464,  0.3405]],\n","\n","        [[-0.4759, -1.1393,  1.0795,  1.1344, -0.4571,  0.4755, -0.3767,\n","           1.4475,  0.0235,  0.1364, -1.0519,  1.9690, -0.7017, -0.2095,\n","          -1.9502,  0.0965]]], grad_fn=<NativeLayerNormBackward0>)\n","Apply attention or feedforward layers.Normalize the output.\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[-1.3913e+00, -5.1160e-01,  3.7123e-01,  8.8107e-01,  7.0920e-01,\n","          -6.7782e-02, -1.2531e+00, -9.3730e-01,  3.7294e-01, -2.6773e-01,\n","          -1.0903e+00, -6.2443e-01,  5.3939e-02,  9.8558e-01, -6.9217e-01,\n","           1.2730e+00]],\n","\n","        [[-1.3042e+00, -6.1074e-01,  5.8130e-01,  6.0030e-01,  4.2178e-01,\n","          -7.4275e-01, -1.3457e+00, -2.1375e-02, -2.7760e-01, -7.8711e-01,\n","          -7.3794e-01, -2.7812e-01, -2.3302e-01,  5.5579e-01,  2.1563e-02,\n","           4.3845e-01]],\n","\n","        [[-1.5178e+00,  5.6203e-02,  9.7219e-01,  1.3070e+00, -3.2047e-02,\n","          -3.2853e-01, -8.7162e-01, -2.6854e-01,  4.0648e-04, -6.2494e-01,\n","          -9.6334e-01, -6.8969e-01, -6.6368e-01,  6.9782e-01, -1.4311e-01,\n","           4.4718e-01]],\n","\n","        [[-1.3821e+00, -4.5664e-01,  1.7792e+00,  3.4363e-01,  5.3642e-01,\n","          -5.9538e-01, -4.6057e-01, -5.0341e-01, -1.3629e-01, -4.7856e-01,\n","          -1.2795e+00, -6.3931e-01, -6.6589e-01,  3.6053e-01,  7.2254e-01,\n","           8.5246e-01]]], grad_fn=<SelectBackward0>)\n","\n","key matrix :-  tensor([[[-0.2308,  0.1133,  0.2903,  0.3853, -0.4516,  0.1959,  0.6090,\n","          -0.3400, -1.2217,  0.6092, -0.3450, -0.2957,  0.5173,  0.6571,\n","          -0.4143, -0.8309]],\n","\n","        [[-0.3502, -0.3782,  0.7167, -0.3702, -0.4569, -0.9139,  0.1453,\n","          -0.1519, -0.0794, -0.1597,  0.1826,  0.0843,  0.3870,  0.8233,\n","          -0.7645, -0.7616]],\n","\n","        [[-0.8141, -0.0045,  1.1946,  0.0737, -0.0576, -0.9219, -0.1994,\n","           0.0913, -0.6467, -0.0184, -0.0388, -0.0290,  0.3467,  0.9112,\n","          -1.1878,  0.0618]],\n","\n","        [[-1.1958,  0.2228,  0.5067,  0.4088,  0.0974, -0.6881,  0.7217,\n","           0.4700, -0.7916,  0.6585,  0.0327, -0.3028,  0.1103, -0.3068,\n","          -0.4121, -0.4361]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[ 0.9274,  0.8024,  0.1087, -0.0837, -0.1602, -0.3736, -0.2765,\n","          -1.1465,  0.4583, -0.5856,  0.5712,  0.5166, -1.4845,  0.5223,\n","           1.1522,  0.1401]],\n","\n","        [[ 1.0722,  0.7900, -0.3487, -0.1540,  0.5873, -0.8652, -0.2027,\n","          -0.8991,  0.1172, -0.7233,  0.7801,  0.8406, -1.2644, -0.3707,\n","           1.1579,  0.2041]],\n","\n","        [[ 1.4356, -0.1186,  0.2294, -0.0721,  0.5492, -0.2558, -0.1447,\n","          -0.8243, -0.0256, -0.5415,  0.5603,  0.0468, -1.2601, -0.4328,\n","           0.8961, -0.6990]],\n","\n","        [[ 0.6633,  0.5436, -1.1095, -0.7482,  0.3112, -0.5089,  0.3097,\n","          -1.0534, -0.1294, -0.3696,  0.6446,  0.5647, -0.7371,  0.2780,\n","           1.7278,  0.3851]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[-1.3913e+00, -5.1160e-01,  3.7123e-01,  8.8107e-01],\n","          [-1.3042e+00, -6.1074e-01,  5.8130e-01,  6.0030e-01],\n","          [-1.5178e+00,  5.6203e-02,  9.7219e-01,  1.3070e+00],\n","          [-1.3821e+00, -4.5664e-01,  1.7792e+00,  3.4363e-01]],\n","\n","         [[ 7.0920e-01, -6.7782e-02, -1.2531e+00, -9.3730e-01],\n","          [ 4.2178e-01, -7.4275e-01, -1.3457e+00, -2.1375e-02],\n","          [-3.2047e-02, -3.2853e-01, -8.7162e-01, -2.6854e-01],\n","          [ 5.3642e-01, -5.9538e-01, -4.6057e-01, -5.0341e-01]],\n","\n","         [[ 3.7294e-01, -2.6773e-01, -1.0903e+00, -6.2443e-01],\n","          [-2.7760e-01, -7.8711e-01, -7.3794e-01, -2.7812e-01],\n","          [ 4.0648e-04, -6.2494e-01, -9.6334e-01, -6.8969e-01],\n","          [-1.3629e-01, -4.7856e-01, -1.2795e+00, -6.3931e-01]],\n","\n","         [[ 5.3939e-02,  9.8558e-01, -6.9217e-01,  1.2730e+00],\n","          [-2.3302e-01,  5.5579e-01,  2.1563e-02,  4.3845e-01],\n","          [-6.6368e-01,  6.9782e-01, -1.4311e-01,  4.4718e-01],\n","          [-6.6589e-01,  3.6053e-01,  7.2254e-01,  8.5246e-01]]]],\n","       grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[-0.2308,  0.1133,  0.2903,  0.3853],\n","          [-0.3502, -0.3782,  0.7167, -0.3702],\n","          [-0.8141, -0.0045,  1.1946,  0.0737],\n","          [-1.1958,  0.2228,  0.5067,  0.4088]],\n","\n","         [[-0.4516,  0.1959,  0.6090, -0.3400],\n","          [-0.4569, -0.9139,  0.1453, -0.1519],\n","          [-0.0576, -0.9219, -0.1994,  0.0913],\n","          [ 0.0974, -0.6881,  0.7217,  0.4700]],\n","\n","         [[-1.2217,  0.6092, -0.3450, -0.2957],\n","          [-0.0794, -0.1597,  0.1826,  0.0843],\n","          [-0.6467, -0.0184, -0.0388, -0.0290],\n","          [-0.7916,  0.6585,  0.0327, -0.3028]],\n","\n","         [[ 0.5173,  0.6571, -0.4143, -0.8309],\n","          [ 0.3870,  0.8233, -0.7645, -0.7616],\n","          [ 0.3467,  0.9112, -1.1878,  0.0618],\n","          [ 0.1103, -0.3068, -0.4121, -0.4361]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[ 0.9274,  0.8024,  0.1087, -0.0837],\n","          [ 1.0722,  0.7900, -0.3487, -0.1540],\n","          [ 1.4356, -0.1186,  0.2294, -0.0721],\n","          [ 0.6633,  0.5436, -1.1095, -0.7482]],\n","\n","         [[-0.1602, -0.3736, -0.2765, -1.1465],\n","          [ 0.5873, -0.8652, -0.2027, -0.8991],\n","          [ 0.5492, -0.2558, -0.1447, -0.8243],\n","          [ 0.3112, -0.5089,  0.3097, -1.0534]],\n","\n","         [[ 0.4583, -0.5856,  0.5712,  0.5166],\n","          [ 0.1172, -0.7233,  0.7801,  0.8406],\n","          [-0.0256, -0.5415,  0.5603,  0.0468],\n","          [-0.1294, -0.3696,  0.6446,  0.5647]],\n","\n","         [[-1.4845,  0.5223,  1.1522,  0.1401],\n","          [-1.2644, -0.3707,  1.1579,  0.2041],\n","          [-1.2601, -0.4328,  0.8961, -0.6990],\n","          [-0.7371,  0.2780,  1.7278,  0.3851]]]], grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[ 0.9274,  0.8024,  0.1087, -0.0837],\n","          [ 1.0043,  0.7958, -0.1343, -0.1210],\n","          [ 1.2230,  0.3205,  0.0733, -0.0928],\n","          [ 1.0837,  0.3574, -0.2772, -0.2816]],\n","\n","         [[-0.1602, -0.3736, -0.2765, -1.1465],\n","          [ 0.3427, -0.7043, -0.2268, -0.9801],\n","          [ 0.3869, -0.5013, -0.1980, -0.9307],\n","          [ 0.3738, -0.5026, -0.0836, -0.9565]],\n","\n","         [[ 0.4583, -0.5856,  0.5712,  0.5166],\n","          [ 0.2965, -0.6509,  0.6702,  0.6702],\n","          [ 0.1896, -0.6128,  0.6315,  0.4565],\n","          [ 0.1276, -0.5542,  0.6297,  0.4765]],\n","\n","         [[-1.4845,  0.5223,  1.1522,  0.1401],\n","          [-1.3704,  0.0596,  1.1552,  0.1732],\n","          [-1.3230, -0.1511,  1.0492, -0.1832],\n","          [-1.1754, -0.0123,  1.2323, -0.0181]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","x (tgt)=  tensor([[[ 0.4540, -1.3520,  0.7491, -0.0715, -0.7477,  0.2000, -0.8822,\n","           1.0342,  0.6194,  1.4088, -0.6975,  2.0692,  0.2639, -1.5740,\n","          -1.0395, -0.4342]],\n","\n","        [[-0.1171, -1.3384,  0.3916, -0.4699, -1.4920,  0.3896, -0.4171,\n","           2.4554,  0.0343,  0.2821,  0.3009,  1.8131,  0.1982, -0.6167,\n","          -1.1325, -0.2815]],\n","\n","        [[-0.5830, -1.1824, -0.1906, -0.7489, -0.2649,  0.4737, -0.1493,\n","           1.8754,  0.7224, -0.4005,  0.2510,  2.3580, -0.0194, -0.8357,\n","          -1.6464,  0.3405]],\n","\n","        [[-0.4759, -1.1393,  1.0795,  1.1344, -0.4571,  0.4755, -0.3767,\n","           1.4475,  0.0235,  0.1364, -1.0519,  1.9690, -0.7017, -0.2095,\n","          -1.9502,  0.0965]]], grad_fn=<NativeLayerNormBackward0>)\n","after attention=  tensor([[[-1.0524,  0.1344,  0.5954, -0.8679,  0.0465,  0.2209,  0.9992,\n","          -0.8082, -0.4453, -1.6158, -0.0410, -0.6408, -0.0886,  0.2973,\n","          -1.2143, -0.1779]],\n","\n","        [[-1.0370,  0.1346,  0.4558, -1.0323,  0.1408,  0.0891,  0.5861,\n","          -0.6115, -0.4301, -1.7481, -0.1814, -0.7376, -0.3421,  0.0038,\n","          -0.7816, -0.3733]],\n","\n","        [[-0.8097,  0.1334,  0.4030, -0.9391, -0.0066,  0.1084,  0.5191,\n","          -0.5550, -0.4391, -1.4678, -0.0622, -0.6504, -0.3171, -0.0922,\n","          -0.7409, -0.5430]],\n","\n","        [[-0.7551,  0.0903,  0.3487, -1.1039,  0.0834,  0.0613,  0.3333,\n","          -0.5272, -0.3576, -1.2592, -0.2539, -0.7107, -0.2179, -0.1136,\n","          -0.6013, -0.5088]]], grad_fn=<ViewBackward0>)\n","after norm1 = \n","tensor([[[-0.3322, -1.0019,  1.7688, -0.7009, -0.4435,  0.7700,  0.4414,\n","           0.5592,  0.5032,  0.0911, -0.4838,  1.8595,  0.5045, -1.0658,\n","          -2.1224, -0.3471]],\n","\n","        [[-0.7627, -0.8109,  1.1758, -1.0999, -0.9538,  0.8187,  0.5187,\n","           2.1410, -0.0283, -1.0649,  0.4708,  1.3967,  0.2156, -0.2386,\n","          -1.4989, -0.2792]],\n","\n","        [[-0.9500, -0.6395,  0.5001, -1.2169,  0.0630,  0.8343,  0.6424,\n","           1.5012,  0.5642, -1.3797,  0.4789,  1.8510,  0.0043, -0.5300,\n","          -1.8486,  0.1253]],\n","\n","        [[-0.8785, -0.6984,  1.7530,  0.3699, -0.0301,  0.8708,  0.2967,\n","           1.2504,  0.0091, -0.7714, -0.9525,  1.5849, -0.5703,  0.0200,\n","          -2.1852, -0.0684]]], grad_fn=<NativeLayerNormBackward0>)\n"," in_projection_packed \n","shapes  =  torch.Size([4, 1, 16]) torch.Size([4, 1, 16]) torch.Size([4, 1, 16])\n","\n","query matrix :-  tensor([[[ 0.4563,  0.0835, -0.7050, -1.1966, -0.5041, -1.1642,  0.0694,\n","           0.4882, -0.1642,  0.2180, -1.3022, -0.3442, -0.7635, -0.1586,\n","          -0.8433,  0.2623]],\n","\n","        [[ 0.3601, -0.2344, -0.6969, -0.9399, -0.9744, -1.8599,  0.2286,\n","           1.0352, -0.8710, -0.4430, -1.4312, -0.2552, -0.5778, -0.6823,\n","          -0.2184, -0.0380]],\n","\n","        [[ 0.3359, -0.1729, -0.7478, -0.4861, -0.6831, -1.7108,  0.4433,\n","           1.1477, -0.6450, -0.0757, -1.1208,  0.0103, -0.9911, -0.6373,\n","          -0.5160, -0.0818]],\n","\n","        [[ 0.2822, -0.1552, -0.2780, -0.6920, -0.5777, -0.9875, -0.2508,\n","           0.9641, -0.4050, -0.0470, -0.5323, -0.2621, -0.9914, -0.2133,\n","          -0.7246,  0.9110]]], grad_fn=<ViewBackward0>)\n","\n","key matrix :-  tensor([[[-0.7819,  0.5173,  0.9198,  0.2087,  0.2855, -0.4022,  0.3856,\n","          -1.3813,  0.6775,  0.3229, -0.0162, -0.5490, -0.1881, -0.1088,\n","           0.5347,  0.0261]],\n","\n","        [[-0.0184,  0.0710,  0.1425,  0.3705,  0.0538, -0.2588,  0.4524,\n","           0.0134,  0.1968, -0.3985, -0.6307, -0.2075,  0.6741,  1.4463,\n","          -0.7200,  0.3493]],\n","\n","        [[-0.3140, -0.0392,  0.9474,  0.0952,  0.8673, -0.5428,  0.8175,\n","          -0.2916,  0.7594, -0.0159, -0.6262,  0.4119, -0.7091,  1.0458,\n","           0.1784, -0.3540]],\n","\n","        [[-0.4381,  0.4917,  0.1487, -0.1290,  0.6306,  0.2761,  0.3763,\n","          -0.2790,  0.1056, -0.4362, -0.6635, -0.1208,  0.5807,  0.2396,\n","           0.0707,  0.7791]]], grad_fn=<SelectBackward0>)\n","\n","value matrix :-  tensor([[[ 0.5504, -0.5492, -0.4369,  0.5605, -0.5224, -0.6666,  0.5882,\n","           0.5236,  0.0328, -0.3889, -0.5228, -0.2153, -1.1177,  0.3919,\n","           1.1746, -1.5159]],\n","\n","        [[ 0.2439,  0.4282, -0.1267, -0.2738,  0.2938, -0.0587,  0.8364,\n","          -0.7360,  0.5640,  0.3953, -0.4410, -1.3444, -0.2797,  0.5616,\n","           0.6664, -0.3570]],\n","\n","        [[ 0.0516,  1.3324,  0.5665,  0.6082, -0.1687, -0.7640, -0.0912,\n","          -0.4752,  0.4350, -0.6321,  0.1679, -0.5070, -0.2495,  1.1642,\n","           0.8030, -0.7646]],\n","\n","        [[ 0.0662, -0.6463, -0.5106,  0.0794, -0.3514, -0.4562,  0.8355,\n","           0.8068,  0.7471,  0.7282,  0.0837, -0.6593, -0.6471,  0.2173,\n","           0.0961, -0.7042]]], grad_fn=<SelectBackward0>)\n","\n","\n","shapes before attention and need_weights is false  :-  torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n","\n","Q_reshaped =  tensor([[[[ 0.4563,  0.0835, -0.7050, -1.1966],\n","          [ 0.3601, -0.2344, -0.6969, -0.9399],\n","          [ 0.3359, -0.1729, -0.7478, -0.4861],\n","          [ 0.2822, -0.1552, -0.2780, -0.6920]],\n","\n","         [[-0.5041, -1.1642,  0.0694,  0.4882],\n","          [-0.9744, -1.8599,  0.2286,  1.0352],\n","          [-0.6831, -1.7108,  0.4433,  1.1477],\n","          [-0.5777, -0.9875, -0.2508,  0.9641]],\n","\n","         [[-0.1642,  0.2180, -1.3022, -0.3442],\n","          [-0.8710, -0.4430, -1.4312, -0.2552],\n","          [-0.6450, -0.0757, -1.1208,  0.0103],\n","          [-0.4050, -0.0470, -0.5323, -0.2621]],\n","\n","         [[-0.7635, -0.1586, -0.8433,  0.2623],\n","          [-0.5778, -0.6823, -0.2184, -0.0380],\n","          [-0.9911, -0.6373, -0.5160, -0.0818],\n","          [-0.9914, -0.2133, -0.7246,  0.9110]]]], grad_fn=<ViewBackward0>)\n","\n","K_reshaped =  tensor([[[[-0.7819,  0.5173,  0.9198,  0.2087],\n","          [-0.0184,  0.0710,  0.1425,  0.3705],\n","          [-0.3140, -0.0392,  0.9474,  0.0952],\n","          [-0.4381,  0.4917,  0.1487, -0.1290]],\n","\n","         [[ 0.2855, -0.4022,  0.3856, -1.3813],\n","          [ 0.0538, -0.2588,  0.4524,  0.0134],\n","          [ 0.8673, -0.5428,  0.8175, -0.2916],\n","          [ 0.6306,  0.2761,  0.3763, -0.2790]],\n","\n","         [[ 0.6775,  0.3229, -0.0162, -0.5490],\n","          [ 0.1968, -0.3985, -0.6307, -0.2075],\n","          [ 0.7594, -0.0159, -0.6262,  0.4119],\n","          [ 0.1056, -0.4362, -0.6635, -0.1208]],\n","\n","         [[-0.1881, -0.1088,  0.5347,  0.0261],\n","          [ 0.6741,  1.4463, -0.7200,  0.3493],\n","          [-0.7091,  1.0458,  0.1784, -0.3540],\n","          [ 0.5807,  0.2396,  0.0707,  0.7791]]]], grad_fn=<ViewBackward0>)\n","\n","V_reshaped =  tensor([[[[ 0.5504, -0.5492, -0.4369,  0.5605],\n","          [ 0.2439,  0.4282, -0.1267, -0.2738],\n","          [ 0.0516,  1.3324,  0.5665,  0.6082],\n","          [ 0.0662, -0.6463, -0.5106,  0.0794]],\n","\n","         [[-0.5224, -0.6666,  0.5882,  0.5236],\n","          [ 0.2938, -0.0587,  0.8364, -0.7360],\n","          [-0.1687, -0.7640, -0.0912, -0.4752],\n","          [-0.3514, -0.4562,  0.8355,  0.8068]],\n","\n","         [[ 0.0328, -0.3889, -0.5228, -0.2153],\n","          [ 0.5640,  0.3953, -0.4410, -1.3444],\n","          [ 0.4350, -0.6321,  0.1679, -0.5070],\n","          [ 0.7471,  0.7282,  0.0837, -0.6593]],\n","\n","         [[-1.1177,  0.3919,  1.1746, -1.5159],\n","          [-0.2797,  0.5616,  0.6664, -0.3570],\n","          [-0.2495,  1.1642,  0.8030, -0.7646],\n","          [-0.6471,  0.2173,  0.0961, -0.7042]]]], grad_fn=<ViewBackward0>)\n","\n","attention output final = \n","tensor([[[[ 0.2016,  0.0874, -0.1603,  0.1926],\n","          [ 0.2030,  0.1201, -0.1450,  0.1929],\n","          [ 0.2076,  0.1240, -0.1459,  0.1859],\n","          [ 0.2120,  0.1409, -0.1297,  0.2209]],\n","\n","         [[-0.1384, -0.4667,  0.5198, -0.0962],\n","          [-0.0872, -0.4360,  0.5187, -0.2003],\n","          [-0.0893, -0.4455,  0.5009, -0.2030],\n","          [-0.1068, -0.4358,  0.5458, -0.1276]],\n","\n","         [[ 0.4753,  0.0767, -0.1629, -0.7200],\n","          [ 0.5207,  0.1785, -0.1518, -0.7777],\n","          [ 0.4985,  0.1188, -0.1511, -0.7454],\n","          [ 0.4703,  0.0864, -0.1748, -0.7171]],\n","\n","         [[-0.5472,  0.6163,  0.6944, -0.8185],\n","          [-0.6316,  0.5847,  0.7313, -0.9248],\n","          [-0.6080,  0.6244,  0.7485, -0.9173],\n","          [-0.5672,  0.5884,  0.6741, -0.8298]]]],\n","       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n","\n","output after attention and normailsation(2) = \n","tensor([[[-0.7312, -0.5803,  1.6808, -0.6923, -0.5460,  0.5523,  0.5362,\n","           0.9027,  0.3131, -0.4261, -0.1242,  2.1134,  0.6660, -1.0855,\n","          -1.7422, -0.8366]],\n","\n","        [[-0.9948, -0.4314,  1.1444, -0.9379, -0.8766,  0.5862,  0.5976,\n","           2.0233, -0.1833, -1.2695,  0.6298,  1.6210,  0.4821, -0.4459,\n","          -1.1724, -0.7725]],\n","\n","        [[-1.1849, -0.3362,  0.6508, -1.0341, -0.0969,  0.5955,  0.6929,\n","           1.5739,  0.3050, -1.5287,  0.6186,  2.0056,  0.3023, -0.6680,\n","          -1.4474, -0.4484]],\n","\n","        [[-1.1496, -0.3530,  1.6834,  0.1874, -0.1869,  0.6619,  0.4255,\n","           1.4558, -0.1009, -1.1478, -0.5215,  1.8833, -0.1893, -0.2193,\n","          -1.8190, -0.6101]]], grad_fn=<NativeLayerNormBackward0>)\n"," after feed forward and normailsation(3) = \n","tensor([[[-0.1540, -1.1039,  2.0197,  0.0417, -0.9670, -0.1161,  0.1252,\n","           1.3772,  0.2856, -0.0096,  0.0075,  1.6997,  0.4651, -1.1117,\n","          -1.5222, -1.0371]],\n","\n","        [[-0.4518, -0.8918,  1.7726, -0.4684, -1.2572, -0.2335,  0.4313,\n","           1.9147, -0.1666, -0.9067,  0.5852,  1.7563,  0.1381, -0.3682,\n","          -0.9018, -0.9523]],\n","\n","        [[-0.7255, -0.6902,  1.5114, -0.7935, -0.5891, -0.0509,  0.3556,\n","           1.7523,  0.3326, -1.1639,  0.4396,  2.1510, -0.0094, -0.6057,\n","          -1.1179, -0.7964]],\n","\n","        [[-0.2212, -0.5768,  2.2412,  0.4771, -0.8093, -0.2258,  0.0908,\n","           1.4760,  0.2336, -0.8543, -0.5968,  1.7080, -0.3397, -0.2077,\n","          -1.6395, -0.7555]]], grad_fn=<NativeLayerNormBackward0>)\n","output (after transformer): torch.Size([4, 1, 16])\n","output:{output}\n"]}]},{"cell_type":"code","source":["print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J71GfZCaU0vm","executionInfo":{"status":"ok","timestamp":1718597230028,"user_tz":-330,"elapsed":451,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"d77b95e8-d4e9-478b-fafb-c295bef5128a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.5040, -0.5643, -0.9291,  0.8937,  0.5021,  0.6562, -1.3417,\n","           0.5391,  0.0286,  0.4028,  0.6045,  0.0248,  0.4549, -0.3143,\n","          -1.1332,  0.7255,  0.1034, -0.2828, -0.1362,  0.0014]],\n","\n","        [[ 0.2908, -0.4276, -1.1432,  0.8511,  0.3242,  0.2948, -1.3010,\n","           0.6434,  0.5933, -0.0025,  0.3320, -0.1345,  0.6465, -0.0753,\n","          -1.1380,  0.1948, -0.0462, -0.3189, -0.3971,  0.0195]],\n","\n","        [[ 0.2088, -0.7398, -1.1280,  0.7504,  0.3378,  0.3129, -1.4142,\n","           0.7165,  0.6326, -0.1854,  0.2747, -0.0866,  0.7972, -0.0041,\n","          -1.0578,  0.4315,  0.0825, -0.2325, -0.3063,  0.0676]],\n","\n","        [[ 0.4189, -0.6181, -0.7970,  1.2109,  0.7027,  0.0376, -0.7485,\n","           0.8167,  0.2986, -0.2507,  0.6037, -0.0616,  1.0253,  0.1367,\n","          -1.2094,  0.7994,  0.3362, -0.8986,  0.2187, -0.1611]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"egHdhL9-U0xv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c2omD_mSU01I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy\n","torch.manual_seed(42)\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512, dropout=0):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)#d_model is the the dimension of the embedding,maxlen-is the maximum length of sequence\n","        position = torch.arange(0, max_len).unsqueeze(1).float() #shape('max_len,1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) #calculates the positional encodings using the formula\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)#even position\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)#odd position\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return self.encoding[:, :x.size(1)].detach()\n","\n","class TransformerModelLogger:\n","  def __init__(self, is_logging=False):\n","    self.is_logging = is_logging\n","    self.logs = []\n","\n","  def log(self, message):\n","    if self.is_logging:\n","      self.logs.append(message)\n","      print(message)\n","\n","  def print_logs(self):\n","     for message in self.logs:\n","\n","      print(message)\n","\n","      self.logs = []\n","\n","class TransformerModel1(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n","\n","        super(TransformerModel1, self).__init__()\n","\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n","\n","        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n","        self.logger = TransformerModelLogger(is_logging=True)\n","\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout,\n","            dim_feedforward=d_ff,\n","        )\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","\n","    def generate_mask(self, src, tgt):\n","\n","        src_mask = None\n","        seq_length = tgt.size(0)\n","\n","        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()#triangular matrix used to mask future positiions of the target sequence\n","\n","        return src_mask, nopeak_mask\n","\n","    def forward(self, src, tgt):\n","\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","\n","        self.logger.log(\"Tgt mask shape = \" + str(tgt_mask.shape))\n","\n","        src = self.src_embedding(src) + self.positional_encoding(src)\n","        self.logger.log(f\"src (after embedding and positional encoding): {src.shape}\")\n","        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n","        self.logger.log(f\"tgt (after embedding and positional encoding): {tgt.shape}\")\n","\n","\n","        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n","        self.logger.log(f\"output (after transformer): {output.shape}\")\n","        output = self.fc(output)\n","        print(\"output:{output}\")\n","\n","        return output\n","\n"],"metadata":{"id":"UTMk0sGEodxj","executionInfo":{"status":"ok","timestamp":1718598340581,"user_tz":-330,"elapsed":652,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512, dropout=0):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)#d_model is the the dimension of the embedding,maxlen-is the maximum length of sequence\n","        position = torch.arange(0, max_len).unsqueeze(1).float() #shape('max_len,1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) #calculates the positional encodings using the formula\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)#even position\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)#odd position\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return self.encoding[:, :x.size(1)].detach()\n"],"metadata":{"id":"x5hAaPAOoNAs","executionInfo":{"status":"ok","timestamp":1718598514954,"user_tz":-330,"elapsed":528,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["def look_up_table(sentence, vocab_embeds, embedding):\n","\n","    for i in range(sentence.size(0)):\n","        for j in range(sentence.size(1)):\n","\n","            # Get the index for the current word token index in the sequence\n","            word_index = sentence[i, j].item()\n","\n","            if word_index < 0 or word_index >= vocab_embeds.size(0):\n","                raise ValueError(f\"Invalid word index: {word_index}\")\n","\n","            # Lookup the corresponding embedding vector for the word\n","            embedding[i, j, :] = vocab_embeds[word_index, :]\n","\n","            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n","    print()\n","\n","    return embedding"],"metadata":{"id":"chm2Hkr2cV5J","executionInfo":{"status":"ok","timestamp":1718598517399,"user_tz":-330,"elapsed":436,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n","\n","    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n","\n","    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n","    print(\"Source sentence embedding\")\n","    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n","    print(src_embedding.shape)\n","\n","    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n","\n","    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n","\n","\n","    print(\"Target sentence embedding\")\n","    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n","\n","    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n","\n","    print(\"PE of src :\")\n","    print(pe(src_sentence))\n","    print()\n","    print(\"PE of tgt :\")\n","    print(pe(tgt_sentence))\n","    print()\n","\n","    pe_src_embeds = src_embedding + pe(src_sentence)\n","\n","    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n","\n","    print(\"PE source embeddings : \\n\")\n","    print(pe_src_embeds)\n","    print()\n","\n","    print(\"PE target embeddings : \\n\")\n","    print(pe_tgt_embeds)\n","    print()\n","\n","    return pe_src_embeds, pe_tgt_embeds"],"metadata":{"id":"2ocL99cGdEyL","executionInfo":{"status":"ok","timestamp":1718598524316,"user_tz":-330,"elapsed":453,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n","\n","    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n","\n","    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n","    print(\"Source sentence embedding\")\n","    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n","    print(src_embedding.shape)\n","\n","    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n","\n","    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n","\n","\n","    print(\"Target sentence embedding\")\n","    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n","\n","    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n","\n","    print(\"PE of src :\")\n","    print(pe(src_sentence))\n","    print()\n","    print(\"PE of tgt :\")\n","    print(pe(tgt_sentence))\n","    print()\n","\n","    pe_src_embeds = src_embedding + pe(src_sentence)\n","\n","    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n","\n","    print(\"PE source embeddings : \\n\")\n","    print(pe_src_embeds)\n","    print()\n","\n","    print(\"PE target embeddings : \\n\")\n","    print(pe_tgt_embeds)\n","    print()\n","\n","    return pe_src_embeds, pe_tgt_embeds"],"metadata":{"id":"CNBeqX1WbxDz","executionInfo":{"status":"ok","timestamp":1718598351031,"user_tz":-330,"elapsed":451,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["src_vocab_size = 20\n","tgt_vocab_size = 20\n","d_model = 16\n","num_heads = 4\n","num_encoder_layers = 2\n","num_decoder_layers = 2\n","d_ff = 20\n","max_seq_len = 5\n","dropout = 0\n","\n","torch.manual_seed(42)\n","model = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)"],"metadata":{"id":"FiZbuCPboj_s","executionInfo":{"status":"ok","timestamp":1718598528622,"user_tz":-330,"elapsed":448,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n","state_dict = model.state_dict()"],"metadata":{"id":"fnfFptquokCp","executionInfo":{"status":"ok","timestamp":1718598534904,"user_tz":-330,"elapsed":694,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","src_data = np.array([[2], [1], [5], [4]])\n","tgt_data = np.array([[1], [16], [5], [3]])\n","\n","src_tensor = torch.tensor(src_data, dtype=torch.long).T\n","tgt_tensor = torch.tensor(tgt_data, dtype=torch.long).T"],"metadata":{"id":"FNKVIavYokFk","executionInfo":{"status":"ok","timestamp":1718598536700,"user_tz":-330,"elapsed":3,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["d_model = 16  # Example dimension of the embeddings, should match d_model used in your Transformer modelReplace with your actual state_dict obtained earlier\n","\n","src_data = np.array([[2], [1], [5], [4]])\n","tgt_data = np.array([[1], [16], [5], [3]])\n","# Retrieve src_embedding weights from state_dict\n","src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n","\n","# Initialize src_embeddings array\n","src_embeddings = np.zeros((src_data.shape[0], d_model))\n","\n","# Iterate over src_data to fill src_embeddings\n","for i in range(src_data.shape[0]):\n","    word_index = src_data[i].item()  # Extract the word index as an integer\n","    if word_index < 0 or word_index >= src_vocab_embeds.shape[0]:\n","        print(f\"Invalid word index: {word_index}\")\n","    else:\n","        src_embeddings[i, :] = src_vocab_embeds[word_index, :].numpy()\n","\n","    # Print the word index and corresponding embedding (for verification)\n","    print(f\"Word index: {word_index}, Embedding: {src_embeddings[i, :]}\")\n","\n","# Print the shape of src_embeddings\n","print(f\"Shape of src_embeddings: {src_embeddings.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_7SOfc0okIL","executionInfo":{"status":"ok","timestamp":1718598231229,"user_tz":-330,"elapsed":11,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"55dc0921-3132-4312-9315-0e3d3ee20067"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Word index: 2, Embedding: [-1.38467371 -0.87123615 -0.22336592  1.71736145  0.31888032 -0.42451897\n","  0.30572093 -0.77459252 -1.55757248  0.99563611 -0.87978584 -0.60114205\n"," -1.27415121  2.12278509 -1.23465312 -0.48791388]\n","Word index: 1, Embedding: [ 1.64231694 -0.15959747 -0.49739754  0.43958926 -0.75813115  1.07831764\n","  0.80080056  1.68062055  1.27912438  1.29642284  0.61046648  1.33473778\n"," -0.23162432  0.04175949 -0.25157529  0.85985851]\n","Word index: 5, Embedding: [ 0.01086814 -0.33874235 -1.34067953 -0.58537054  0.53618813  0.52462262\n","  1.14120162  0.0516436   0.74395198 -0.4815844  -1.04946613  0.60389882\n"," -1.72229505 -0.82776886  1.33470297  0.48353928]\n","Word index: 4, Embedding: [ 1.44513381  0.85641253  2.21807575  0.52316552  0.34664667 -0.19733144\n"," -1.05458891  1.27799559 -0.17219013  0.52378845  0.05662182  0.42629614\n","  0.57500505 -0.64172411 -2.20639849 -0.75080305]\n","Shape of src_embeddings: (4, 16)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","def positional_encoding(max_len, d_model):\n","\n","    pos_encodings = np.zeros((max_len, d_model))\n","    for pos in range(max_len):\n","        for i in range(0, d_model, 2):\n","            pos_encodings[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n","            pos_encodings[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","    return pos_encodings"],"metadata":{"id":"GvGf58zvow2D","executionInfo":{"status":"ok","timestamp":1718597495134,"user_tz":-330,"elapsed":521,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["pos_encodings=positional_encoding(4,16)\n","final_src=pos_encodings + src_embeddings"],"metadata":{"id":"lgyxOUM3ow4-","executionInfo":{"status":"ok","timestamp":1718597497956,"user_tz":-330,"elapsed":447,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["enc=final_src"],"metadata":{"id":"ITmr3Lgzow8I","executionInfo":{"status":"ok","timestamp":1718597498663,"user_tz":-330,"elapsed":3,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def calculate_qkv(query, key, value ,W, b):\n","\n","\n","    E = query.size(-1)\n","\n","    if key is value:\n","        if query is key:\n","            tempop1 = query@W.T\n","            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n","            return tempop1[0], tempop1[1], tempop1[2]\n","        else:\n","            W_q, W_kv = W.split([E, E * 2])\n","            if b is None:\n","                b_q = b_kv = None\n","            else:\n","                b_q, b_kv = b.split([E, E * 2])\n","            q_matmul = query@W_q.T\n","            kv_matmul = key@W_kv.T\n","            print(kv_matmul)\n","\n","            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n","            return q_matmul, kv_matmul[0], kv_matmul[1]\n","    else:\n","\n","        W_q, W_k, W_v = W.chunk(3)\n","        if b is None:\n","            b_q = b_k = b_v = None\n","        else:\n","            b_q, b_k, b_v = b.chunk(3)\n","        q_matmul = query@W_q.T\n","        k_matmul = key@W_k.T\n","        v_matmul = value@W_v.T\n","        return q_matmul, k_matmul, v_matmul"],"metadata":{"id":"jMHozJRtow_g","executionInfo":{"status":"ok","timestamp":1718598363131,"user_tz":-330,"elapsed":420,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["def attention_calculate(Q, V, K, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads):\n","    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n","    K1 = K.view(bsz, num_heads, src_len, head_dim)\n","    V1 = V.view(bsz, num_heads, src_len, head_dim)\n","\n","    L, S = Q1.size(-2), K1.size(-2)\n","\n","    scale_factor = 1 / math.sqrt(Q1.size(-1))\n","    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n","\n","    if attn_mask is not None:\n","        if attn_mask.dtype == torch.bool:\n","            masked_tensor = attn_mask.float().masked_fill(attn_mask, float('-inf'))\n","            masked_tensor = masked_tensor.masked_fill(~attn_mask, 0)\n","            attn_mask = masked_tensor\n","            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n","            attn_bias += attn_mask\n","\n","        else:\n","            attn_bias += attn_mask\n","            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n","    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n","    attn_weight += attn_bias\n","    attn_weight = torch.softmax(attn_weight, dim=-1)\n","\n","    print(\"Attention_weights after softmax= \", attn_weight)\n","    print()\n","    sum_last_dim = attn_weight.sum(dim=-1)\n","    tolerance = 1e-6\n","    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n","\n","    print(attn_weight)\n","    attn_output = attn_weight @ V1\n","\n","    print(\"final_attention_value after multiplication with Venc= \", attn_output)\n","    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n","\n","    print(\"Attention output = \")\n","    print(attn_weight.shape, attn_weight)\n","\n","    return attn_output\n","\n","def attention_calculate_if_needweightstrue(Q, K, V, bsz, tgt_len, embed_dim, attn_mask):\n","    B, Nt, E = Q.shape\n","    Q_scaled = Q / math.sqrt(E)\n","\n","    if attn_mask is not None:\n","        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n","    else:\n","        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n","\n","    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n","    attn_output = torch.bmm(attn_wt_matrix, V)\n","    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n","\n","\n","    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n","    tolerance = 1e-6\n","    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n","\n","    print(\"Encoder Attention output = \")\n","    print(attn_output)\n","    print()\n","\n","    return attn_output, attn_wt_matrix"],"metadata":{"id":"SfE9HQVoI5qq","executionInfo":{"status":"ok","timestamp":1718598365557,"user_tz":-330,"elapsed":428,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["def encoder_block_attention_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n","    query_enc = key_enc = value_enc = x\n","    tgt_len, bsz, embed_dim = x.shape\n","\n","    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n","    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n","\n","    head_dim = embed_dim//num_heads\n","    Q_enc,K_enc,V_enc = calculate_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n","\n","    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n","    K_enc = K_enc.reshape(K_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","    V_enc = V_enc.reshape(V_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","    print(\"Query_{} = \".format(layer_num))\n","    print(Q_enc)\n","    print()\n","\n","    print(\"Key_{} = \".format(layer_num))\n","    print(K_enc)\n","    print()\n","\n","    print(\"Value_enc_{} = \".format(layer_num))\n","    print(V_enc)\n","    print()\n","\n","\n","    src_len = K_enc.size(1)\n","\n","    attn_mask = src_mask\n","    if attn_mask is not None:\n","        if attn_mask.dim() == 2:\n","            correct_2d_size = (tgt_len, src_len)\n","            if attn_mask.shape != correct_2d_size:\n","                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n","            attn_mask = attn_mask.unsqueeze(0)\n","        elif attn_mask.dim() == 3:\n","            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n","            if attn_mask.shape != correct_3d_size:\n","                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n","        else:\n","            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n","    if attn_mask is not None:\n","        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n","            attn_mask = attn_mask.unsqueeze(0)\n","        else:\n","            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n","\n","\n","    if need_weights is False:\n","\n","        attn_output = attention_calculate(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads=num_heads)\n","\n","        return attn_output, src_len, head_dim, None\n","\n","    else:\n","\n","        attn_enc_output,attn_wt_matrix_enc = attention_calculate_if_needweightstrue(Q_enc, K_enc, V_enc, bsz, tgt_len, embed_dim,  attn_mask)\n","\n","        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n"],"metadata":{"id":"IM-NAxmRI5t2","executionInfo":{"status":"ok","timestamp":1718598368303,"user_tz":-330,"elapsed":459,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n","    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n","    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n","    output_enc_1 = attn_enc_output + x\n","    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n","\n","    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n","    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n","\n","    x = linear_result_enc_1\n","    w = layernorm_enc_1.weight\n","    b = layernorm_enc_1.bias\n","\n","    linear_result_enc_1f = w*x + b\n","\n","    epsilon = 1e-05\n","    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n","    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n","    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n","    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n","    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n","    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n","    output_enc_2 = op_enc_2 + linear_op_enc_1\n","    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n","    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n","    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n","    x = output_enc_2_norm\n","    w = layernorm_enc_final.weight\n","    b = layernorm_enc_final.bias\n","\n","    linear_result_enc_2 = w*x + b\n","\n","    print(linear_result_enc_2)\n","    epsilon = 1e-05\n","    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n","    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n","    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n","\n","\n","    print(\"final_encoder_output :\".format(layer_num))\n","    print(\"after feed forward and norm2 is applied\")\n","    print(output_enc_final)\n","    print()\n","    return output_enc_final"],"metadata":{"id":"2ze2SeiyJcfh","executionInfo":{"status":"ok","timestamp":1718598371809,"user_tz":-330,"elapsed":488,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["def decoder_block_self_attn_output(x, state_dict, layer_num, num_heads, tgt_mask = None,need_weights = False):\n","    query_dec = key_dec = value_dec = x\n","    tgt_len, bsz, embed_dim = x.shape\n","    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n","    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n","\n","\n","    head_dim = embed_dim//num_heads\n","    Q_dec,K_dec,V_dec = calculate_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n","    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n","    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","\n","    print(\"Q_dec_{} = \".format(layer_num))\n","    print(Q_dec)\n","    print()\n","    print(\"K_dec_{} = \".format(layer_num))\n","    print(K_dec)\n","    print()\n","    print(\"V_dec_{} = \".format(layer_num))\n","    print(V_dec)\n","    print()\n","\n","    src_len = K_dec.size(1)\n","\n","\n","    attn_mask = tgt_mask\n","    if attn_mask is not None:\n","        if attn_mask.dim() == 2:\n","            correct_2d_size = (tgt_len, src_len)\n","            if attn_mask.shape != correct_2d_size:\n","                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n","            attn_mask = attn_mask.unsqueeze(0)\n","        elif attn_mask.dim() == 3:\n","            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n","            if attn_mask.shape != correct_3d_size:\n","                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n","        else:\n","            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n","    if attn_mask is not None:\n","        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n","            attn_mask = attn_mask.unsqueeze(0)\n","        else:\n","            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n","\n","\n","\n","    if need_weights is False:\n","        attn_output = attention_calculate(Q = Q_dec, V = V_dec, K = K_dec, bsz = bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim, num_heads=num_heads\n","        )\n","\n","        print(\"decoder = \")\n","        print(attn_output)\n","        print()\n","\n","        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n","        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n","\n","        return attn_dec_output, None\n","\n","    else:\n","\n","        attn_dec_output,attn_wt_matrix_dec = attention_calculate_if_needweightstrue(Q=Q_dec, K=K_dec, V=V_dec, bsz=bsz, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n","\n","        print(\"attention if needweights is true = \")\n","        print(attn_wt_matrix_dec)\n","        print()\n","\n","        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n","        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n","\n","\n","        return attn_dec_output, attn_wt_matrix_dec\n"],"metadata":{"id":"uxHOVe0xJciT","executionInfo":{"status":"ok","timestamp":1718598374424,"user_tz":-330,"elapsed":460,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n","    output_dec_1 = self_attn_dec + x\n","    print(output_dec_1)\n","    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n","\n","    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n","    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n","    x = linear_result_dec_1\n","\n","    w = layernorm_dec_1.weight\n","    b = layernorm_dec_1.bias\n","\n","    linear_result_dec_1f = w*x + b\n","\n","    epsilon = 1e-05\n","    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n","    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n","\n","    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n","\n","\n","    print(\"decoder after layernorm1\".format(layer_num))\n","    print(normalized_result_dec_1)\n","    print()\n","    return normalized_result_dec_1"],"metadata":{"id":"RAiyPoRJJclL","executionInfo":{"status":"ok","timestamp":1718598253738,"user_tz":-330,"elapsed":531,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, num_heads, memory_mask = None,need_weights = False):\n","\n","    print(\"next input from the encoder= \",memory)\n","    print()\n","    query_dec_mha = x_dec\n","    key_dec_mha, value_dec_mha = memory, memory\n","    tgt_len, bsz, embed_dim = query_dec_mha.shape\n","\n","    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n","    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n","    Q_dec_mha,K_dec_mha,V_dec_mha = calculate_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n","    Q_dec_mha = Q_dec_mha.reshape(Q_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","    K_dec_mha = K_dec_mha.reshape(K_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","    V_dec_mha = V_dec_mha.reshape(V_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n","\n","    print(\"Q_dec_{} = \".format(layer_num))\n","    print(Q_dec_mha)\n","    print()\n","    print(\"K_dec_{} = \".format(layer_num))\n","    print(K_dec_mha)\n","    print()\n","    print(\"V_dec_{} = \".format(layer_num))\n","    print(V_dec_mha)\n","    print()\n","\n","\n","    attn_mask = memory_mask\n","    if attn_mask is not None:\n","        if attn_mask.dim() == 2:\n","            correct_2d_size = (tgt_len, src_len)\n","            if attn_mask.shape != correct_2d_size:\n","                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n","            attn_mask = attn_mask.unsqueeze(0)\n","        elif attn_mask.dim() == 3:\n","            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n","            if attn_mask.shape != correct_3d_size:\n","                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n","        else:\n","            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n","    if attn_mask is not None:\n","        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n","            attn_mask = attn_mask.unsqueeze(0)\n","        else:\n","            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n","\n","\n","\n","    if need_weights is False:\n","\n","        attn_output_dec_mha = attention_calculate(Q =Q_dec_mha, V=V_dec_mha, K=K_dec_mha, bsz=bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim, num_heads=num_heads)\n","        print(\"after cross_attention_{}\".format(layer_num))\n","        print(attn_output_dec_mha)\n","        print()\n","\n","        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n","        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n","\n","        return attn_dec_mha_output, None\n","\n","    else:\n","      attn_dec_mha_output ,attn_wt_matrix_dec_mha = attention_calculate_if_needweightstrue(Q=Q_dec_mha, K=K_dec_mha, V=V_dec_mha, bsz=bsz, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim)\n","      print(\"decoder attention_output = \")\n","      print(attn_dec_mha_output)\n","      print()\n","\n","\n","      op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n","      attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n","\n","\n","      return attn_dec_output_mha , attn_wt_matrix_dec_mha\n"],"metadata":{"id":"cpD6pEy7Jcns","executionInfo":{"status":"ok","timestamp":1718598380090,"user_tz":-330,"elapsed":500,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n","    output_dec_2 = attn_dec_mha_output + x_dec\n","    print(output_dec_2)\n","    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n","    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n","    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n","\n","    x = linear_result_dec_2\n","    w = layernorm_dec_2.weight\n","    b = layernorm_dec_2.bias\n","\n","    linear_result_dec_2f = w*x + b\n","\n","    epsilon = 1e-05\n","    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n","    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n","    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n","    print((linear_result_dec_2f - mean))\n","    print(\"after norm2\")\n","    print(normalized_result_dec_2)\n","    x_dec2_norm = normalized_result_dec_2\n","\n","    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n","\n","    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n","    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n","\n","\n","    ff_dec = op_dec_2\n","\n","    x_dec3_unorm = x_dec2_norm + ff_dec\n","\n","    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n","\n","    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n","    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n","    x = linear_result_dec_3\n","    w = layernorm_dec_3.weight\n","    b = layernorm_dec_3.bias\n","\n","    linear_result_dec_3f = w*x + b\n","\n","    epsilon = 1e-5\n","    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n","    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n","    print( (linear_result_dec_3f - mean))\n","    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n","    print(\"after norm3\")\n","    print(normalized_result_dec_3)\n","    print()\n","    return normalized_result_dec_3\n","\n","def feedforward(dec_output_final, state_dict):\n","    W_ff=state_dict[\"fc.weight\"]\n","    b_ff=state_dict[\"fc.bias\"]\n","    final_op = dec_output_final@W_ff.T + b_ff\n","\n","    return final_op"],"metadata":{"id":"etN4rUNyJcqQ","executionInfo":{"status":"ok","timestamp":1718598258993,"user_tz":-330,"elapsed":534,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512, dropout=0):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        print(x.shape)\n","        return self.encoding[:, :x.size(1)].detach()\n","\n","\n","\n","class TransformerModel1(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n","\n","        super(TransformerModel1, self).__init__()\n","\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","\n","        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n","\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout,\n","            dim_feedforward=d_ff,\n","        )\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","\n","    def generate_mask(self, src, tgt):\n","\n","        src_mask = None\n","        seq_length = tgt.size(0)\n","\n","        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n","\n","        return src_mask, nopeak_mask\n","\n","    def forward(self, src, tgt):\n","\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","\n","        print(\"Tgt mask shape = \", tgt_mask.shape)\n","\n","        src = self.src_embedding(src) + self.positional_encoding(src)\n","        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n","\n","\n","        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n","        output = self.fc(output)\n","\n","        return output\n"],"metadata":{"id":"lJjgAM_DJctD","executionInfo":{"status":"ok","timestamp":1718598386828,"user_tz":-330,"elapsed":385,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","src_vocab_size = 20\n","tgt_vocab_size = 20\n","d_model = 16\n","num_heads = 4\n","num_encoder_layers = 2\n","num_decoder_layers = 2\n","d_ff = 20\n","max_seq_len = 5\n","dropout = 0\n","\n","\n","transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n","\n","\n","src_data = torch.tensor([[2], [1], [5], [4]])\n","tgt_data = torch.tensor([[1], [16], [5], [3]])\n","\n","state_dict = transformer.state_dict()"],"metadata":{"id":"Res_t1r9JcwM","executionInfo":{"status":"ok","timestamp":1718598266682,"user_tz":-330,"elapsed":717,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","state_dict1 = copy.deepcopy(state_dict)"],"metadata":{"id":"f8NxogRZJczF","executionInfo":{"status":"ok","timestamp":1718598267902,"user_tz":-330,"elapsed":5,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["def generate_mask(src, tgt):\n","    src_mask = None\n","    # Convert tgt to a PyTorch tensor if it's not already\n","    if not isinstance(tgt, torch.Tensor):\n","        tgt = torch.tensor(tgt)\n","    seq_length = tgt.size(0)\n","    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n","\n","    return src_mask, nopeak_mask\n","\n","src_mask, tgt_mask = generate_mask(src_data, tgt_data)"],"metadata":{"id":"HJSX2qNWI5v9","executionInfo":{"status":"ok","timestamp":1718598429046,"user_tz":-330,"elapsed":425,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def get_all_intermediate_outputs_mask(src_sentence, tgt_sentence,d_model, num_heads ,state_dict, num_encoder_layers , num_decoder_layers, tgt_mask, d_ff):\n","\n","    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n","    print(\"encoder\")\n","    print()\n","    x_enc = pe_src_embeds\n","\n","    for lno in range(num_encoder_layers):\n","        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attention_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n","\n","        if lno == 0:\n","            tgt_len, bsz, embed_dim = x_enc.shape\n","\n","        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n","\n","        x_enc = output_enc_final\n","    print(\"decoder\")\n","    print()\n","\n","    x_dec = pe_tgt_embeds\n","    memory = x_enc\n","\n","    for lno in range(num_decoder_layers):\n","\n","        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=tgt_mask, num_heads = num_heads)\n","        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n","\n","        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, num_heads = num_heads, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n","        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n","\n","        print(pe_tgt_embeds.shape, final_op.shape)\n","        x_dec = final_op\n","\n","    final_op = feedforward(final_op, state_dict)\n","\n","    return final_op\n"],"metadata":{"id":"gdPrgjztKI7P","executionInfo":{"status":"ok","timestamp":1718598433035,"user_tz":-330,"elapsed":646,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["\n","src_mask, tgt_mask = generate_mask(src_data, tgt_data)"],"metadata":{"id":"3OP6yPu-KI-M","executionInfo":{"status":"ok","timestamp":1718598435901,"user_tz":-330,"elapsed":496,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["state_dict.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qU-l7I0Za-2","executionInfo":{"status":"ok","timestamp":1718598437668,"user_tz":-330,"elapsed":10,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"c7c62f00-5ee7-4b35-d0c7-4f9439432294"},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['src_embedding.weight', 'tgt_embedding.weight', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc.weight', 'fc.bias'])"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["final_op = get_all_intermediate_outputs_mask(src_data, tgt_data, state_dict = state_dict1, num_heads = num_heads, num_encoder_layers = 2 , num_decoder_layers = 2, d_model=d_model,  d_ff = d_ff, tgt_mask = tgt_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"1s5-WzlQKJDp","executionInfo":{"status":"error","timestamp":1718598554821,"user_tz":-330,"elapsed":598,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"4f5c458e-679d-4947-d6eb-fd17086fe5ea"},"execution_count":101,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'int' object is not callable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-101-0408ee9a7421>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_intermediate_outputs_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0md_ff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-89-6e8a4fee9720>\u001b[0m in \u001b[0;36mget_all_intermediate_outputs_mask\u001b[0;34m(src_sentence, tgt_sentence, d_model, num_heads, state_dict, num_encoder_layers, num_decoder_layers, tgt_mask, d_ff)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_intermediate_outputs_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpe_src_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe_tgt_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtgt_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-96-bd90279ff0dc>\u001b[0m in \u001b[0;36mget_embedding_outputs\u001b[0;34m(src_sentence, tgt_sentence, max_seq_len, state_dict, d_model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msrc_vocab_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_embedding.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msrc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source sentence embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msrc_embedding\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlook_up_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"]}]},{"cell_type":"code","source":["final_op"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qmmkl7uVKJG9","executionInfo":{"status":"ok","timestamp":1718435491196,"user_tz":-330,"elapsed":439,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"b3876fb2-db9c-455f-eb47-da911d1ffa50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.5040, -0.5643, -0.9291,  0.8937,  0.5021,  0.6562, -1.3417,\n","           0.5391,  0.0286,  0.4028,  0.6045,  0.0248,  0.4549, -0.3143,\n","          -1.1331,  0.7255,  0.1034, -0.2828, -0.1362,  0.0014]],\n","\n","        [[ 0.2908, -0.4276, -1.1431,  0.8511,  0.3242,  0.2948, -1.3010,\n","           0.6434,  0.5933, -0.0025,  0.3320, -0.1345,  0.6465, -0.0753,\n","          -1.1380,  0.1948, -0.0462, -0.3189, -0.3971,  0.0195]],\n","\n","        [[ 0.2088, -0.7398, -1.1280,  0.7504,  0.3378,  0.3129, -1.4142,\n","           0.7165,  0.6326, -0.1854,  0.2747, -0.0866,  0.7972, -0.0041,\n","          -1.0578,  0.4315,  0.0825, -0.2325, -0.3063,  0.0676]],\n","\n","        [[ 0.4189, -0.6181, -0.7970,  1.2109,  0.7027,  0.0376, -0.7485,\n","           0.8167,  0.2986, -0.2507,  0.6037, -0.0616,  1.0253,  0.1367,\n","          -1.2094,  0.7994,  0.3362, -0.8986,  0.2187, -0.1611]]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":[],"metadata":{"id":"8jOTrrt3I5yV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#verification\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy\n","torch.manual_seed(42)\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512, dropout=0):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)#d_model is the the dimension of the embedding,maxlen-is the maximum length of sequence\n","        position = torch.arange(0, max_len).unsqueeze(1).float() #shape('max_len,1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)) #calculates the positional encodings using the formula\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)#even position\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)#odd position\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return self.encoding[:, :x.size(1)].detach()\n","\n","class TransformerModelLogger:\n","  def __init__(self, is_logging=False):\n","    self.is_logging = is_logging\n","    self.logs = []\n","\n","  def log(self, message):\n","    if self.is_logging:\n","      self.logs.append(message)\n","      print(message)\n","\n","  def print_logs(self):\n","     for message in self.logs:\n","\n","      print(message)\n","\n","      self.logs = []\n","\n","class TransformerModel1(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n","\n","        super(TransformerModel1, self).__init__()\n","\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n","\n","        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n","        self.logger = TransformerModelLogger(is_logging=True)\n","\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout,\n","            dim_feedforward=d_ff,\n","        )\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","\n","    def generate_mask(self, src, tgt):\n","\n","        src_mask = None\n","        seq_length = tgt.size(0)\n","\n","        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()#triangular matrix used to mask future positiions of the target sequence\n","\n","        return src_mask, nopeak_mask\n","\n","    def forward(self, src, tgt):\n","\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","\n","        self.logger.log(\"Tgt mask shape = \" + str(tgt_mask.shape))\n","\n","        src = self.src_embedding(src) + self.positional_encoding(src)\n","        self.logger.log(f\"src (after embedding and positional encoding): {src.shape}\")\n","        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n","        self.logger.log(f\"tgt (after embedding and positional encoding): {tgt.shape}\")\n","\n","\n","        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n","        self.logger.log(f\"output (after transformer): {output.shape}\")\n","        output = self.fc(output)\n","        print(\"output:{output}\")\n","\n","        return output\n","\n"],"metadata":{"id":"s6KrjjikI517"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","src_vocab_size = 20\n","tgt_vocab_size = 20\n","d_model = 16\n","num_heads = 4\n","num_encoder_layers = 2\n","num_decoder_layers = 2\n","d_ff = 20\n","max_seq_len = 5\n","dropout = 0\n","\n","transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n","\n","\n","\n","src_data = torch.tensor([[2], [1], [5], [4]])\n","tgt_data = torch.tensor([[1], [16], [5], [3]])\n","\n","\n","state_dict = transformer.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lplKN5EK8vJs","executionInfo":{"status":"ok","timestamp":1718506200859,"user_tz":-330,"elapsed":596,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"d50e6b76-f171-434f-9da7-05010ada023c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["import copy\n","\n","state_dict1 = copy.deepcopy(state_dict)"],"metadata":{"id":"sUPD883n8vax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","transformer.train()\n","\n","\n","\n","output = transformer(src_data, tgt_data)\n","\n","\n","print(\"output shape = \",output.shape)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB6T8TGw83Tw","executionInfo":{"status":"ok","timestamp":1718506206802,"user_tz":-330,"elapsed":651,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"ed359064-f72e-40c8-96c9-fb1934fd61c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tgt mask shape = torch.Size([4, 4])\n","src (after embedding and positional encoding): torch.Size([4, 1, 16])\n","tgt (after embedding and positional encoding): torch.Size([4, 1, 16])\n","output (after transformer): torch.Size([4, 1, 16])\n","output:{output}\n","output shape =  torch.Size([4, 1, 20])\n"]}]},{"cell_type":"code","source":["print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pQD-OEb83Zr","executionInfo":{"status":"ok","timestamp":1718506217163,"user_tz":-330,"elapsed":460,"user":{"displayName":"chaturya ganne","userId":"10364823250779832102"}},"outputId":"212687dc-2aa8-4dd5-c084-836f2733c3ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.5040, -0.5643, -0.9291,  0.8937,  0.5021,  0.6562, -1.3417,\n","           0.5391,  0.0286,  0.4028,  0.6045,  0.0248,  0.4549, -0.3143,\n","          -1.1332,  0.7255,  0.1034, -0.2828, -0.1362,  0.0014]],\n","\n","        [[ 0.2908, -0.4276, -1.1432,  0.8511,  0.3242,  0.2948, -1.3010,\n","           0.6434,  0.5933, -0.0025,  0.3320, -0.1345,  0.6465, -0.0753,\n","          -1.1380,  0.1948, -0.0462, -0.3189, -0.3971,  0.0195]],\n","\n","        [[ 0.2088, -0.7398, -1.1280,  0.7504,  0.3378,  0.3129, -1.4142,\n","           0.7165,  0.6326, -0.1854,  0.2747, -0.0866,  0.7972, -0.0041,\n","          -1.0578,  0.4315,  0.0825, -0.2325, -0.3063,  0.0676]],\n","\n","        [[ 0.4189, -0.6181, -0.7970,  1.2109,  0.7027,  0.0376, -0.7485,\n","           0.8167,  0.2986, -0.2507,  0.6037, -0.0616,  1.0253,  0.1367,\n","          -1.2094,  0.7994,  0.3362, -0.8986,  0.2187, -0.1611]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9N4buSbv83cO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_TEW8Qf583fn"},"execution_count":null,"outputs":[]}]}